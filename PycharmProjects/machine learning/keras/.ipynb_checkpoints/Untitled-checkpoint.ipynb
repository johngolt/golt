{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layer import Layer\n",
    "class OurLayer(Layer):\n",
    "    \"\"\"定义新的Layer，增加reuse方法，允许在定义Layer时调用现成的层\n",
    "    \"\"\"\n",
    "    def reuse(self, layer, *args, **kwargs):\n",
    "        if not layer.built:\n",
    "            if len(args) > 0:\n",
    "                inputs = args[0]\n",
    "            else:\n",
    "                inputs = kwargs['inputs']\n",
    "            if isinstance(inputs, list):\n",
    "                input_shape = [K.int_shape(x) for x in inputs]\n",
    "            else:\n",
    "                input_shape = K.int_shape(inputs)\n",
    "            layer.build(input_shape)\n",
    "        outputs = layer.call(*args, **kwargs)\n",
    "        for w in layer.trainable_weights:\n",
    "            if w not in self._trainable_weights:\n",
    "                self._trainable_weights.append(w)\n",
    "        for w in layer.non_trainable_weights:\n",
    "            if w not in self._non_trainable_weights:\n",
    "                self._non_trainable_weights.append(w)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个 OurLayer 类继承了原来的 Layer 类，为它增加了 reuse 方法，就是通过它我们可以重用已有的层。$y=g\\left(f\\left(x W_{1}+b_{1}\\right) W_{2}+b_{2}\\right)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurDense(OurLayer):\n",
    "    \"\"\"原来是继承Layer类，现在继承OurLayer类\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dimdim, output_dim,\n",
    "                 hidden_activation='linear',\n",
    "                 output_activation='linear', **kwargs):\n",
    "        super(OurDense, self).__init__(**kwargs)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.output_activation = output_activation\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"在build方法里边添加需要重用的层，\n",
    "        当然也可以像标准写法一样条件可训练的权重。\n",
    "        \"\"\"\n",
    "        super(OurDense, self).build(input_shape)\n",
    "        self.h_dense = Dense(self.hidden_dim,\n",
    "                             activation=self.hidden_activation)\n",
    "        self.o_dense = Dense(self.output_dim,\n",
    "                             activation=self.output_activation)\n",
    "    def call(self, inputs):\n",
    "        \"\"\"直接reuse一下层，等价于o_dense(h_dense(inputs))\n",
    "        \"\"\"\n",
    "        h = self.reuse(self.h_dense, inputs)\n",
    "        o = self.reuse(self.o_dense, h)\n",
    "        return o\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1] + (self.output_dim,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurBidirectional(OurLayer):\n",
    "    \"\"\"自己封装双向RNN，允许传入mask，保证对齐\n",
    "    \"\"\"\n",
    "    def __init__(self, layer, **args):\n",
    "        super(OurBidirectional, self).__init__(**args)\n",
    "        self.forward_layer = copy.deepcopy(layer)\n",
    "        self.backward_layer = copy.deepcopy(layer)\n",
    "        self.forward_layer.name = 'forward_' + self.forward_layer.name\n",
    "        self.backward_layer.name = 'backward_' + self.backward_layer.name\n",
    "    def reverse_sequence(self, x, mask):\n",
    "        \"\"\"这里的mask.shape是[batch_size, seq_len, 1]\n",
    "        \"\"\"\n",
    "        seq_len = K.round(K.sum(mask, 1)[:, 0])\n",
    "        seq_len = K.cast(seq_len, 'int32')\n",
    "        return K.tf.reverse_sequence(x, seq_len, seq_dim=1)\n",
    "    def call(self, inputs):\n",
    "        x, mask = inputs\n",
    "        x_forward = self.reuse(self.forward_layer, x)\n",
    "        x_backward = self.reverse_sequence(x, mask)\n",
    "        x_backward = self.reuse(self.backward_layer, x_backward)\n",
    "        x_backward = self.reverse_sequence(x_backward, mask)\n",
    "        x = K.concatenate([x_forward, x_backward], 2)\n",
    "        return x * mask\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, input_shape[0][1], self.forward_layer.units * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型重用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型克隆\n",
    "from keras.models import clone_model\n",
    "'''clone_model 完全复制了原模型模型的结构，并重新构建了一个模型，\n",
    "但没有复制原模型的权重的值。也就是说，\n",
    "对于同样的输入，model1.predict 和 model2.predict 的结果是不一样的。'''\n",
    "model2 = clone_model(model1)\n",
    "model2.set_weights(K.batch_get_value(model1.weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''这里的交叉引用是指在定义一个新层的时候，沿用已有的某个层的权重，\n",
    "注意这个自定义层可能跟旧层的功能完全不一样，它们之间纯粹是共享了某个权重而已。'''\n",
    "class EmbeddingDense(Layer):\n",
    "    \"\"\"运算跟Dense一致，只不过kernel用Embedding层的embedding矩阵\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_layer, activation='softmax', **kwargs):\n",
    "        super(EmbeddingDense, self).__init__(**kwargs)\n",
    "        self.kernel = K.transpose(embedding_layer.embeddings)\n",
    "        self.activation = activation\n",
    "        self.units = K.int_shape(self.kernel)[1]\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(EmbeddingDense, self).build(input_shape)\n",
    "        self.bias = self.add_weight(name='bias',\n",
    "                                    shape=(self.units,),\n",
    "                                    initializer='zeros')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        outputs = K.dot(inputs, self.kernel)\n",
    "        outputs = K.bias_add(outputs, self.bias)\n",
    "        outputs = Activation(self.activation).call(outputs)\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1] + (self.units,)\n",
    "\n",
    "\n",
    "# 用法\n",
    "embedding_layer = Embedding(10000, 128)\n",
    "x = embedding_layer(x) # 调用Embedding层\n",
    "x = EmbeddingDense(embedding_layer)(x) # 调用EmbeddingDense层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outputs_of(model, start_tensors, input_layers=None):\n",
    "    \"\"\"start_tensors为开始拆开的位置\n",
    "    \"\"\"\n",
    "    # 为此操作建立新模型\n",
    "    model = Model(inputs=model.input,\n",
    "                  outputs=model.output,\n",
    "                  name='outputs_of_' + model.name)\n",
    "    # 适配工作，方便使用\n",
    "    if not isinstance(start_tensors, list):\n",
    "        start_tensors = [start_tensors]\n",
    "    if input_layers is None:\n",
    "        input_layers = [\n",
    "            Input(shape=K.int_shape(x)[1:], dtype=K.dtype(x))\n",
    "            for x in start_tensors\n",
    "        ]\n",
    "    elif not isinstance(input_layers, list):\n",
    "        input_layers = [input_layers]\n",
    "    # 核心：覆盖模型的输入\n",
    "    model.inputs = start_tensors\n",
    "    model._input_layers = [x._keras_history[0] for x in input_layers]\n",
    "    # 适配工作，方便使用\n",
    "    if len(input_layers) == 1:\n",
    "        input_layers = input_layers[0]\n",
    "    # 整理层，参考自 Model 的 run_internal_graph 函数\n",
    "    layers, tensor_map = [], set()\n",
    "    for x in model.inputs:\n",
    "        tensor_map.add(str(id(x)))\n",
    "    depth_keys = list(model._nodes_by_depth.keys())\n",
    "    depth_keys.sort(reverse=True)\n",
    "    for depth in depth_keys:\n",
    "        nodes = model._nodes_by_depth[depth]\n",
    "        for node in nodes:\n",
    "            n = 0\n",
    "            for x in node.input_tensors:\n",
    "                if str(id(x)) in tensor_map:\n",
    "                    n += 1\n",
    "            if n == len(node.input_tensors):\n",
    "                if node.outbound_layer not in layers:\n",
    "                    layers.append(node.outbound_layer)\n",
    "                for x in node.output_tensors:\n",
    "                    tensor_map.add(str(id(x)))\n",
    "    model._layers = layers # 只保留用到的层\n",
    "    # 计算输出\n",
    "    outputs = model(input_layers)\n",
    "    return input_layers, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    " \n",
    "# 保证结果的复现\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    " \n",
    "np.random.seed(42)\n",
    " \n",
    "rn.seed(12345)\n",
    " \n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    " \n",
    "from keras import backend as K\n",
    " \n",
    "tf.set_random_seed(1234)\n",
    " \n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    " \n",
    " \n",
    "''' \n",
    "输入数据是32*2，32个样本，2个类别特征，且类别特征的可能值是0到9之间（10个）。\n",
    "对这2个特征做one-hot的话，应该为32*20，\n",
    "embedding就是使1个特征原本应该one-hot的10维变为3维（手动设定，也可以是其它），因为有2个类别特征\n",
    "这样输出的结果就应该是32*6'''\n",
    "model = Sequential()\n",
    "model.add(Embedding(10, 3, input_length=2))\n",
    " \n",
    "# 构造输入数据\n",
    "input_array = np.random.randint(10, size=(32, 2))\n",
    " \n",
    "# 搭建模型\n",
    "model.compile('rmsprop', 'mse')\n",
    " \n",
    "# 得到输出数据 输出格式为32*2*3。我们最终想要的格式为32*6，其实就是把2*3按照行拉成6维，然后就是我们对类别特征进行\n",
    "# embedding后得到的结果了。\n",
    "output_array = model.predict(input_array)\n",
    "# 查看权重参数\n",
    "weight = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "model = ResNet50(weights='imagenet')\n",
    "\n",
    "x, y = get_outputs_of(\n",
    "    model,\n",
    "    model.get_layer('add_15').output\n",
    ")\n",
    "\n",
    "model2 = Model(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#代码有点长，但其实逻辑很简单，真正核心的代码只有三行：\n",
    "\n",
    "model.inputs = start_tensors\n",
    "model._input_layers = [x._keras_history[0] for x in input_layers]\n",
    "outputs = model(input_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# input layer\n",
    "input_layer = Input(shape=(28, 28, 1))\n",
    "\n",
    "# encoding architecture\n",
    "encoded_layer1 = Conv2D(64, (3, 3), activation='relu', padding='same')(input_layer)\n",
    "encoded_layer1 = MaxPool2D( (2, 2), padding='same')(encoded_layer1)\n",
    "encoded_layer2 = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded_layer1)\n",
    "encoded_layer2 = MaxPool2D( (2, 2), padding='same')(encoded_layer2)\n",
    "encoded_layer3 = Conv2D(16, (3, 3), activation='relu', padding='same')(encoded_layer2)\n",
    "latent_view    = MaxPool2D( (2, 2), padding='same')(encoded_layer3)\n",
    "\n",
    "# decoding architecture\n",
    "decoded_layer1 = Conv2D(16, (3, 3), activation='relu', padding='same')(latent_view)\n",
    "decoded_layer1 = UpSampling2D((2, 2))(decoded_layer1)\n",
    "decoded_layer2 = Conv2D(32, (3, 3), activation='relu', padding='same')(decoded_layer1)\n",
    "decoded_layer2 = UpSampling2D((2, 2))(decoded_layer2)\n",
    "decoded_layer3 = Conv2D(64, (3, 3), activation='relu')(decoded_layer2)\n",
    "decoded_layer3 = UpSampling2D((2, 2))(decoded_layer3)\n",
    "output_layer   = Conv2D(1, (3, 3), padding='same')(decoded_layer3)\n",
    "\n",
    "# compile the model\n",
    "model_2 = Model(input_layer, output_layer)\n",
    "model_2.compile(optimizer='adam', loss='mse')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=5, mode='auto')\n",
    "history = model_2.fit(train_x_n, train_x, epochs=10, batch_size=2048, validation_data=(val_x_n, val_x), callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "## input layer\n",
    "input_layer = Input(shape=(784,))\n",
    "\n",
    "## encoding architecture\n",
    "encode_layer1 = Dense(1500, activation='relu')(input_layer)\n",
    "encode_layer2 = Dense(1000, activation='relu')(encode_layer1)\n",
    "encode_layer3 = Dense(500, activation='relu')(encode_layer2)\n",
    "\n",
    "## latent view\n",
    "latent_view   = Dense(10, activation='sigmoid')(encode_layer3)\n",
    "\n",
    "## decoding architecture\n",
    "decode_layer1 = Dense(500, activation='relu')(latent_view)\n",
    "decode_layer2 = Dense(1000, activation='relu')(decode_layer1)\n",
    "decode_layer3 = Dense(1500, activation='relu')(decode_layer2)\n",
    "\n",
    "## output layer\n",
    "output_layer  = Dense(784)(decode_layer3)\n",
    "\n",
    "model = Model(input_layer, output_layer)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "model.fit(train_x, train_x, epochs=20, batch_size=2048, validation_data=(val_x, val_x), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
