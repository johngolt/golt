{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input,Dense,Conv1D,Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''如果层仅有一个计算节点（即该层不是共享层），则可以通过下列方法获得输入张量、输出张量、输入数据的形状和输出数据的形状：\n",
    "layer.input\n",
    "layer.output\n",
    "layer.input_shape\n",
    "layer.output_shape\n",
    "如果该层有多个计算节点（参考层计算节点和共享层）。可以使用下面的方法\n",
    "layer.get_input_at(node_index)\n",
    "layer.get_output_at(node_index)\n",
    "layer.get_input_shape_at(node_index)\n",
    "layer.get_output_shape_at(node_index)'''\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = Input(shape = (78,))\n",
    "x = Dense(32)(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 常用层 keras.layers.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_3/BiasAdd:0' shape=(?, 32) dtype=float32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dense\n",
    "from keras.models import Sequential\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape=(16,)))\n",
    "'''units：大于0的整数，代表该层的输出维度\n",
    " now the model will take as input arrays of shape (*, 16)\n",
    " and output arrays of shape (*, 32)'''\n",
    "model.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation层\n",
    "keras.layers.core.Activation(activation)\n",
    "#activation：将要使用的激活函数，为预定义激活函数名或一个Tensorflow/Theano的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropout层\n",
    "#Dropout将在训练过程中每次更新参数时按一定概率（rate）随机断开输入神经元，Dropout层用于防止过拟合。\n",
    "keras.layers.core.Dropout(rate, noise_shape=None, seed=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten 层\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(64, 3, 3,\n",
    "            border_mode='same',\n",
    "            input_shape=(3, 32, 32)))\n",
    "# now: model.output_shape == (None, 64, 32, 32)\n",
    "\n",
    "model.add(Flatten())\n",
    "# now: model.output_shape == (None, 65536)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape 层\n",
    "'''输入shape\n",
    "任意，但输入的shape必须固定。当使用该层为模型首层时，需要指定input_shape参数\n",
    "输出shape\n",
    "(batch_size,)+target_shape\n",
    "'''\n",
    "# as first layer in a Sequential model\n",
    "model = Sequential()\n",
    "model.add(Reshape((3, 4), input_shape=(12,)))\n",
    "# now: model.output_shape == (None, 3, 4)\n",
    "# note: `None` is the batch dimension\n",
    "\n",
    "# as intermediate layer in a Sequential model\n",
    "model.add(Reshape((6, 2)))\n",
    "# now: model.output_shape == (None, 6, 2)\n",
    "\n",
    "# also supports shape inference using `-1` as dimension\n",
    "model.add(Reshape((-1, 2, 2)))\n",
    "# now: model.output_shape == (None, 3, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permute层\n",
    "#Permute层将输入的维度按照给定模式进行重排，例如，当需要将RNN和CNN网络连接时，可能会用到该层。\n",
    "model = Sequential()\n",
    "model.add(Permute((2, 1), input_shape=(10, 64)))\n",
    "# now: model.output_shape == (None, 64, 10)\n",
    "# note: `None` is the batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RepeatVector层\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=32))\n",
    "# now: model.output_shape == (None, 32)\n",
    "# note: `None` is the batch dimension\n",
    "model.add(RepeatVector(3))\n",
    "# now: model.output_shape == (None, 3, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda 层\n",
    "keras.layers.core.Lambda(function, output_shape=None, mask=None, arguments=None)\n",
    "#arguments：可选，字典，用来记录向函数中传递的其他关键字参数\n",
    "def antirectifier(x):\n",
    "    x -= K.mean(x, axis=1, keepdims=True)\n",
    "    x = K.l2_normalize(x, axis=1)\n",
    "    pos = K.relu(x)\n",
    "    neg = K.relu(-x)\n",
    "    return K.concatenate([pos, neg], axis=1)\n",
    "\n",
    "def antirectifier_output_shape(input_shape):\n",
    "    shape = list(input_shape)\n",
    "    assert len(shape) == 2  # only valid for 2D tensors\n",
    "    shape[-1] *= 2\n",
    "    return tuple(shape)\n",
    "\n",
    "model.add(Lambda(antirectifier,\n",
    "         output_shape=antirectifier_output_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv1D\n",
    "keras.layers.convolutional.Conv1D(filters, kernel_size, strides=1, padding='valid', dilation_rate=1, activation=None, use_bias=True,\n",
    "                                  kernel_initializer='glorot_uniform', \n",
    "                                  bias_initializer='zeros', kernel_regularizer=None,\n",
    "                                  bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
    "\n",
    "'''可以将Convolution1D看作Convolution2D的快捷版，\n",
    "对例子中（10，32）的信号进行1D卷积相当于对其进行卷积核为（filter_length, 32）的2D卷积。'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv2D\n",
    "'''二维卷积层，即对图像的空域卷积。该层对二维输入进行滑动窗卷积，\n",
    "当使用该层作为第一层时，应提供input_shape参数。\n",
    "例如input_shape = (128,128,3)代表128*128的彩色RGB图像\n",
    "\n",
    "输入shape\n",
    "‘channels_first’模式下，输入形如（samples,channels，rows，cols）的4D张量\n",
    "‘channels_last’模式下，输入形如（samples，rows，cols，channels）的4D张量\n",
    "\n",
    "‘channels_first’模式下，为形如（samples，nb_filter, new_rows, new_cols）\n",
    "的4D张量\n",
    "‘channels_last’模式下，为形如（samples，new_rows, new_cols，nb_filter）\n",
    "的4D张量\n",
    "'''\n",
    "\n",
    "# SeparableConv2D 层\n",
    "'''该层是在深度方向上的可分离卷积。\n",
    "\n",
    "可分离卷积首先按深度方向进行卷积（对每个输入通道分别卷积），\n",
    "然后逐点进行卷积，将上一步的卷积结果混合到输出通道中。\n",
    "参数depth_multiplier控制了在depthwise卷积（第一步）的过程中，\n",
    "每个输入通道信号产生多少个输出通道。'''\n",
    "\n",
    "# Conv2DTranspose 层\n",
    "'''该层是转置的卷积操作（反卷积）。\n",
    "需要反卷积的情况通常发生在用户想要对一个普通卷积的结果做反方向的变换。\n",
    "例如，将具有该卷积层输出shape的tensor转换为具有该卷积层输入shape的tensor。\n",
    "同时保留与卷积层兼容的连接模式。'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv3D\n",
    "'''三维卷积对三维的输入进行滑动窗卷积，当使用该层作为第一层时，\n",
    "应提供input_shape参数。例如input_shape = (3,10,128,128)\n",
    "代表对10帧128*128的彩色RGB图像进行卷积\n",
    "输入shape\n",
    "‘channels_first’模式下，输入应为形如\n",
    "（samples，channels，input_dim1，input_dim2, input_dim3）的5D张量\n",
    "\n",
    "‘channels_last’模式下，输入应为形如\n",
    "（samples，input_dim1，input_dim2, input_dim3，channels）的5D张量\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cropping1D 层\n",
    "# 在时间轴(axis1)上对1D输入进行裁剪\n",
    "# Cropping2D\n",
    "#对2D输入进行裁剪，将在空域维度，集宽和高的方向上裁剪\n",
    "#Cropping3D \n",
    "# 对2D输入进行裁剪\n",
    "#ZeroPadding/Unsampling1/2/3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 池化层  keras.layers.pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MaxPooling1D\n",
    "'''输入shape\n",
    "形如（samples，steps，features）的3D张量\n",
    "输出shape\n",
    "形如（samples，downsampled_steps，features）的3D张量'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 局部连接层 keras.layers.local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LocallyConnected1D\n",
    "'''LocallyConnected1D层与Conv1D工作方式类似，\n",
    "唯一的区别是不进行权值共享。\n",
    "即施加在不同输入位置的滤波器是不一样的。'''\n",
    "\n",
    "#LocallyConnected2D\n",
    "'''LocallyConnected2D层与Convolution2D工作方式类似，\n",
    "唯一的区别是不进行权值共享。即施加在不同输入patch的滤波器是不一样的，\n",
    "当使用该层作为模型首层时，需要提供参数input_dim或input_shape参数。\n",
    "参数含义参考Convolution2D'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 循环层 keras.layers.recurrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent 层\n",
    "'''这是循环层的抽象类，\n",
    "请不要在模型中直接应用该层（因为它是抽象类，无法实例化任何对象）。\n",
    "请使用它的子类LSTM，GRU或SimpleRNN。\n",
    "所有的循环层（LSTM,GRU,SimpleRNN）都继承本层，\n",
    "因此下面的参数可以在任何循环层中使用。\n",
    "输入shape\n",
    "形如（samples，timesteps，input_dim）的3D张量\n",
    "输出shape\n",
    "如果return_sequences=True：\n",
    "返回形如（samples，timesteps，output_dim）的3D张量\n",
    "否则，返回形如（samples，output_dim）的2D张量\n",
    "'''\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(10, 64)))\n",
    "# now model.output_shape == (None, 32)\n",
    "# note: `None` is the batch dimension.\n",
    "\n",
    "# the following is identical:\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_dim=64, input_length=10))\n",
    "\n",
    "# for subsequent layers, no need to specify the input size:\n",
    "model.add(LSTM(16))\n",
    "\n",
    "# to stack recurrent layers, you must use return_sequences=True\n",
    "# on any recurrent layer that feeds into another recurrent layer.\n",
    "# note that you only need to specify the input size on the first layer.\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_dim=64, input_length=10, return_sequences=True))\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(LSTM(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimpleRNN层\n",
    "# 全连接RNN网络，RNN的输出会被回馈到输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU 门限循环单元\n",
    "#LSTM 长短期记忆模型\n",
    "# ConvLSTM2D\n",
    "#ConvLSTM2D是一个LSTM网络，但它的输入变换和循环变换是通过卷积实现的\n",
    "# SimpleRNNCell  SimpleRNN的Cell类\n",
    "# GRUCell\n",
    "#LSTMCell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 嵌入层 keras.layers.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding\n",
    "'''嵌入层将正整数（下标）转换为具有固定大小的向量，\n",
    "如[[4],[20]]->[[0.25,0.1],[0.6,-0.2]]\n",
    "Embedding层只能作为模型的第一层\n",
    "输入shape\n",
    "形如（samples，sequence_length）的2D张量\n",
    "输出shape\n",
    "形如(samples, sequence_length, output_dim)的3D张量\n",
    "'''\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(1000, 64, input_length=10))\n",
    "# the model will take as input an integer matrix of size (batch, input_length).\n",
    "# the largest integer (i.e. word index) in the input should be no larger than 999 (vocabulary size).\n",
    "# now model.output_shape == (None, 10, 64), where None is the batch dimension.\n",
    "\n",
    "input_array = np.random.randint(1000, size=(32, 10))\n",
    "\n",
    "model.compile('rmsprop', 'mse')\n",
    "output_array = model.predict(input_array)\n",
    "assert output_array.shape == (32, 10, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
