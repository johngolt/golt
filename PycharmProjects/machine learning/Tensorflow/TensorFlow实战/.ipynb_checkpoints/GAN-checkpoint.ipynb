{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T14:40:56.810237Z",
     "start_time": "2019-05-25T14:40:56.806243Z"
    }
   },
   "source": [
    "#### TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Deep Convolutional Generative Adversarial Network (DCGAN).\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "# Training Params\n",
    "num_steps = 20000\n",
    "batch_size = 32\n",
    "\n",
    "# Network Params\n",
    "image_dim = 784 # 28*28 pixels * 1 channel\n",
    "gen_hidden_dim = 256\n",
    "disc_hidden_dim = 256\n",
    "noise_dim = 200 # Noise data points\n",
    "\n",
    "# Generator Network\n",
    "# Input: Noise, Output: Image\n",
    "def generator(x, reuse=False):\n",
    "    with tf.variable_scope('Generator', reuse=reuse):\n",
    "        # TensorFlow Layers automatically create variables and calculate their\n",
    "        # shape, based on the input.\n",
    "        x = tf.layers.dense(x, units=6 * 6 * 128)\n",
    "        x = tf.nn.tanh(x)\n",
    "        # Reshape to a 4-D array of images: (batch, height, width, channels)\n",
    "        # New shape: (batch, 6, 6, 128)\n",
    "        x = tf.reshape(x, shape=[-1, 6, 6, 128])\n",
    "        # Deconvolution, image shape: (batch, 14, 14, 64)\n",
    "        x = tf.layers.conv2d_transpose(x, 64, 4, strides=2)\n",
    "        # Deconvolution, image shape: (batch, 28, 28, 1)\n",
    "        x = tf.layers.conv2d_transpose(x, 1, 2, strides=2)\n",
    "        # Apply sigmoid to clip values between 0 and 1\n",
    "        x = tf.nn.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Discriminator Network\n",
    "# Input: Image, Output: Prediction Real/Fake Image\n",
    "def discriminator(x, reuse=False):\n",
    "    with tf.variable_scope('Discriminator', reuse=reuse):\n",
    "        # Typical convolutional neural network to classify images.\n",
    "        x = tf.layers.conv2d(x, 64, 5)\n",
    "        x = tf.nn.tanh(x)\n",
    "        x = tf.layers.average_pooling2d(x, 2, 2)\n",
    "        x = tf.layers.conv2d(x, 128, 5)\n",
    "        x = tf.nn.tanh(x)\n",
    "        x = tf.layers.average_pooling2d(x, 2, 2)\n",
    "        x = tf.contrib.layers.flatten(x)\n",
    "        x = tf.layers.dense(x, 1024)\n",
    "        x = tf.nn.tanh(x)\n",
    "        # Output 2 classes: Real and Fake images\n",
    "        x = tf.layers.dense(x, 2)\n",
    "    return x\n",
    "\n",
    "# Build Networks\n",
    "# Network Inputs\n",
    "noise_input = tf.placeholder(tf.float32, shape=[None, noise_dim])\n",
    "real_image_input = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])\n",
    "\n",
    "# Build Generator Network\n",
    "gen_sample = generator(noise_input)\n",
    "\n",
    "# Build 2 Discriminator Networks (one from noise input, one from generated samples)\n",
    "disc_real = discriminator(real_image_input)\n",
    "disc_fake = discriminator(gen_sample, reuse=True)\n",
    "disc_concat = tf.concat([disc_real, disc_fake], axis=0)\n",
    "\n",
    "# Build the stacked generator/discriminator\n",
    "stacked_gan = discriminator(gen_sample, reuse=True)\n",
    "\n",
    "# Build Targets (real or fake images)\n",
    "disc_target = tf.placeholder(tf.int32, shape=[None])\n",
    "gen_target = tf.placeholder(tf.int32, shape=[None])\n",
    "\n",
    "# Build Loss\n",
    "disc_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    logits=disc_concat, labels=disc_target))\n",
    "gen_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    logits=stacked_gan, labels=gen_target))\n",
    "\n",
    "# Build Optimizers\n",
    "optimizer_gen = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "optimizer_disc = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "\n",
    "# Training Variables for each optimizer\n",
    "# By default in TensorFlow, all variables are updated by each optimizer, so we\n",
    "# need to precise for each one of them the specific variables to update.\n",
    "# Generator Network Variables\n",
    "gen_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Generator')\n",
    "# Discriminator Network Variables\n",
    "disc_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Discriminator')\n",
    "\n",
    "# Create training operations\n",
    "train_gen = optimizer_gen.minimize(gen_loss, var_list=gen_vars)\n",
    "train_disc = optimizer_disc.minimize(disc_loss, var_list=disc_vars)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    for i in range(1, num_steps+1):\n",
    "\n",
    "        # Prepare Input Data\n",
    "        # Get the next batch of MNIST data (only images are needed, not labels)\n",
    "        batch_x, _ = mnist.train.next_batch(batch_size)\n",
    "        batch_x = np.reshape(batch_x, newshape=[-1, 28, 28, 1])\n",
    "        # Generate noise to feed to the generator\n",
    "        z = np.random.uniform(-1., 1., size=[batch_size, noise_dim])\n",
    "\n",
    "        # Prepare Targets (Real image: 1, Fake image: 0)\n",
    "        # The first half of data fed to the generator are real images,\n",
    "        # the other half are fake images (coming from the generator).\n",
    "        batch_disc_y = np.concatenate(\n",
    "            [np.ones([batch_size]), np.zeros([batch_size])], axis=0)\n",
    "        # Generator tries to fool the discriminator, thus targets are 1.\n",
    "        batch_gen_y = np.ones([batch_size])\n",
    "\n",
    "        # Training\n",
    "        feed_dict = {real_image_input: batch_x, noise_input: z,\n",
    "                     disc_target: batch_disc_y, gen_target: batch_gen_y}\n",
    "        _, _, gl, dl = sess.run([train_gen, train_disc, gen_loss, disc_loss],\n",
    "                                feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import UpSampling2D\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.core import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import math\n",
    "\n",
    "\n",
    "def generator_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_dim=100, output_dim=1024))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dense(128*7*7))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Reshape((128, 7, 7), input_shape=(128*7*7,)))\n",
    "    model.add(UpSampling2D(size=(2, 2)))\n",
    "    model.add(Convolution2D(64, 5, 5, border_mode='same'))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(UpSampling2D(size=(2, 2)))\n",
    "    model.add(Convolution2D(1, 5, 5, border_mode='same'))\n",
    "    model.add(Activation('tanh'))\n",
    "    return model\n",
    "\n",
    "\n",
    "def discriminator_model():\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(\n",
    "                        64, 5, 5,\n",
    "                        border_mode='same',\n",
    "                        input_shape=(1, 28, 28)))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Convolution2D(128, 5, 5))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    return model\n",
    "\n",
    "\n",
    "def generator_containing_discriminator(generator, discriminator):\n",
    "    model = Sequential()\n",
    "    model.add(generator)\n",
    "    discriminator.trainable = False\n",
    "    model.add(discriminator)\n",
    "    return model\n",
    "\n",
    "\n",
    "def combine_images(generated_images):\n",
    "    num = generated_images.shape[0]\n",
    "    width = int(math.sqrt(num))\n",
    "    height = int(math.ceil(float(num)/width))\n",
    "    shape = generated_images.shape[2:]\n",
    "    image = np.zeros((height*shape[0], width*shape[1]),\n",
    "                     dtype=generated_images.dtype)\n",
    "    for index, img in enumerate(generated_images):\n",
    "        i = int(index/width)\n",
    "        j = index % width\n",
    "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = \\\n",
    "            img[0, :, :]\n",
    "    return image\n",
    "\n",
    "\n",
    "def train(BATCH_SIZE):\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    X_train = (X_train.astype(np.float32) - 127.5)/127.5\n",
    "    X_train = X_train.reshape((X_train.shape[0], 1) + X_train.shape[1:])\n",
    "    discriminator = discriminator_model()\n",
    "    generator = generator_model()\n",
    "    discriminator_on_generator = \\\n",
    "        generator_containing_discriminator(generator, discriminator)\n",
    "    d_optim = SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
    "    g_optim = SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
    "    generator.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n",
    "    discriminator_on_generator.compile(\n",
    "        loss='binary_crossentropy', optimizer=g_optim)\n",
    "    discriminator.trainable = True\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=d_optim)\n",
    "    noise = np.zeros((BATCH_SIZE, 100))\n",
    "    for epoch in range(100):\n",
    "        print(\"Epoch is\", epoch)\n",
    "        print(\"Number of batches\", int(X_train.shape[0]/BATCH_SIZE))\n",
    "        for index in range(int(X_train.shape[0]/BATCH_SIZE)):\n",
    "            for i in range(BATCH_SIZE):\n",
    "                noise[i, :] = np.random.uniform(-1, 1, 100)\n",
    "            image_batch = X_train[index*BATCH_SIZE:(index+1)*BATCH_SIZE]\n",
    "            generated_images = generator.predict(noise, verbose=0)\n",
    "            if index % 20 == 0:\n",
    "                image = combine_images(generated_images)\n",
    "                image = image*127.5+127.5\n",
    "                Image.fromarray(image.astype(np.uint8)).save(\n",
    "                    str(epoch)+\"_\"+str(index)+\".png\")\n",
    "            X = np.concatenate((image_batch, generated_images))\n",
    "            y = [1] * BATCH_SIZE + [0] * BATCH_SIZE\n",
    "            d_loss = discriminator.train_on_batch(X, y)\n",
    "            print(\"batch %d d_loss : %f\" % (index, d_loss))\n",
    "            for i in range(BATCH_SIZE):\n",
    "                noise[i, :] = np.random.uniform(-1, 1, 100)\n",
    "            discriminator.trainable = False\n",
    "            g_loss = discriminator_on_generator.train_on_batch(\n",
    "                noise, [1] * BATCH_SIZE)\n",
    "            discriminator.trainable = True\n",
    "            print(\"batch %d g_loss : %f\" % (index, g_loss))\n",
    "            if index % 10 == 9:\n",
    "                generator.save_weights('generator', True)\n",
    "                discriminator.save_weights('discriminator', True)\n",
    "\n",
    "\n",
    "def generate(BATCH_SIZE, nice=False):\n",
    "    generator = generator_model()\n",
    "    generator.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n",
    "    generator.load_weights('generator')\n",
    "    if nice:\n",
    "        discriminator = discriminator_model()\n",
    "        discriminator.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n",
    "        discriminator.load_weights('discriminator')\n",
    "        noise = np.zeros((BATCH_SIZE*20, 100))\n",
    "        for i in range(BATCH_SIZE*20):\n",
    "            noise[i, :] = np.random.uniform(-1, 1, 100)\n",
    "        generated_images = generator.predict(noise, verbose=1)\n",
    "        d_pret = discriminator.predict(generated_images, verbose=1)\n",
    "        index = np.arange(0, BATCH_SIZE*20)\n",
    "        index.resize((BATCH_SIZE*20, 1))\n",
    "        pre_with_index = list(np.append(d_pret, index, axis=1))\n",
    "        pre_with_index.sort(key=lambda x: x[0], reverse=True)\n",
    "        nice_images = np.zeros((BATCH_SIZE, 1) +\n",
    "                               (generated_images.shape[2:]), dtype=np.float32)\n",
    "        for i in range(int(BATCH_SIZE)):\n",
    "            idx = int(pre_with_index[i][1])\n",
    "            nice_images[i, 0, :, :] = generated_images[idx, 0, :, :]\n",
    "        image = combine_images(nice_images)\n",
    "    else:\n",
    "        noise = np.zeros((BATCH_SIZE, 100))\n",
    "        for i in range(BATCH_SIZE):\n",
    "            noise[i, :] = np.random.uniform(-1, 1, 100)\n",
    "        generated_images = generator.predict(noise, verbose=1)\n",
    "        image = combine_images(generated_images)\n",
    "    image = image*127.5+127.5\n",
    "    Image.fromarray(image.astype(np.uint8)).save(\n",
    "        \"generated_image.png\")\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--mode\", type=str)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=128)\n",
    "    parser.add_argument(\"--nice\", dest=\"nice\", action=\"store_true\")\n",
    "    parser.set_defaults(nice=False)\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = get_args()\n",
    "    if args.mode == \"train\":\n",
    "        train(BATCH_SIZE=args.batch_size)\n",
    "    elif args.mode == \"generate\":\n",
    "        generate(BATCH_SIZE=args.batch_size, nice=args.nice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "# This line allows mpl to run with no DISPLAY defined\n",
    "mpl.use('Agg')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.layers import Reshape, Flatten, LeakyReLU, Activation, Dense, BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D, UpSampling2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.regularizers import L1L2\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras_adversarial.image_grid_callback import ImageGridCallback\n",
    "\n",
    "from keras_adversarial import AdversarialModel, simple_gan, gan_targets\n",
    "from keras_adversarial import AdversarialOptimizerSimultaneous, normal_latent_sampling\n",
    "import keras.backend as K\n",
    "from cifar10_utils import cifar10_data\n",
    "from image_utils import dim_ordering_fix, dim_ordering_unfix, dim_ordering_shape\n",
    "\n",
    "\n",
    "def model_generator():\n",
    "    model = Sequential()\n",
    "    nch = 256\n",
    "    reg = lambda: L1L2(l1=1e-7, l2=1e-7)\n",
    "    h = 5\n",
    "    model.add(Dense(nch * 4 * 4, input_dim=100, kernel_regularizer=reg()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Reshape(dim_ordering_shape((nch, 4, 4))))\n",
    "    model.add(Conv2D(int(nch / 2), (h, h), padding='same', kernel_regularizer=reg()))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(UpSampling2D(size=(2, 2)))\n",
    "    model.add(Conv2D(int(nch / 2), (h, h), padding='same', kernel_regularizer=reg()))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(UpSampling2D(size=(2, 2)))\n",
    "    model.add(Conv2D(int(nch / 4), (h, h), padding='same', kernel_regularizer=reg()))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(UpSampling2D(size=(2, 2)))\n",
    "    model.add(Conv2D(3, (h, h), padding='same', kernel_regularizer=reg()))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    return model\n",
    "\n",
    "\n",
    "def model_discriminator():\n",
    "    nch = 256\n",
    "    h = 5\n",
    "    reg = lambda: L1L2(l1=1e-7, l2=1e-7)\n",
    "\n",
    "    c1 = Conv2D(int(nch / 4), (h, h), padding='same', kernel_regularizer=reg(),\n",
    "                input_shape=(32, 32, 3))\n",
    "    c2 = Conv2D(int(nch / 2), (h, h), padding='same', kernel_regularizer=reg())\n",
    "    c3 = Conv2D(nch, (h, h), padding='same', kernel_regularizer=reg())\n",
    "    c4 = Conv2D(1, (h, h), padding='same', kernel_regularizer=reg())\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(c1)\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(c2)\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(c3)\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(c4)\n",
    "    model.add(AveragePooling2D(pool_size=(4, 4), padding='valid'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Activation('sigmoid'))\n",
    "    return model\n",
    "\n",
    "\n",
    "def example_gan(adversarial_optimizer, path, opt_g, opt_d, nb_epoch, generator, discriminator, latent_dim,\n",
    "                targets=gan_targets, loss='binary_crossentropy'):\n",
    "    csvpath = os.path.join(path, \"history.csv\")\n",
    "    if os.path.exists(csvpath):\n",
    "        print(\"Already exists: {}\".format(csvpath))\n",
    "        return\n",
    "\n",
    "    print(\"Training: {}\".format(csvpath))\n",
    "    # gan (x - > yfake, yreal), z is gaussian generated on GPU\n",
    "    # can also experiment with uniform_latent_sampling\n",
    "    generator.summary()\n",
    "    discriminator.summary()\n",
    "    gan = simple_gan(generator=generator,\n",
    "                     discriminator=discriminator,\n",
    "                     latent_sampling=normal_latent_sampling((latent_dim,)))\n",
    "\n",
    "    # build adversarial model\n",
    "    model = AdversarialModel(base_model=gan,\n",
    "                             player_params=[generator.trainable_weights, discriminator.trainable_weights],\n",
    "                             player_names=[\"generator\", \"discriminator\"])\n",
    "    model.adversarial_compile(adversarial_optimizer=adversarial_optimizer,\n",
    "                              player_optimizers=[opt_g, opt_d],\n",
    "                              loss=loss)\n",
    "\n",
    "    # create callback to generate images\n",
    "    zsamples = np.random.normal(size=(10 * 10, latent_dim))\n",
    "\n",
    "    def generator_sampler():\n",
    "        xpred = dim_ordering_unfix(generator.predict(zsamples)).transpose((0, 2, 3, 1))\n",
    "        return xpred.reshape((10, 10) + xpred.shape[1:])\n",
    "\n",
    "    generator_cb = ImageGridCallback(os.path.join(path, \"epoch-{:03d}.png\"), generator_sampler, cmap=None)\n",
    "\n",
    "    # train model\n",
    "    xtrain, xtest = cifar10_data()\n",
    "    y = targets(xtrain.shape[0])\n",
    "    ytest = targets(xtest.shape[0])\n",
    "    callbacks = [generator_cb]\n",
    "    if K.backend() == \"tensorflow\":\n",
    "        callbacks.append(\n",
    "            TensorBoard(log_dir=os.path.join(path, 'logs'), histogram_freq=0, write_graph=True, write_images=True))\n",
    "    history = model.fit(x=xtrain, y=y, validation_data=(xtest, ytest),\n",
    "                        callbacks=callbacks, nb_epoch=nb_epoch,\n",
    "                        batch_size=32)\n",
    "\n",
    "    # save history to CSV\n",
    "    df = pd.DataFrame(history.history)\n",
    "    df.to_csv(csvpath)\n",
    "\n",
    "    # save models\n",
    "    generator.save(os.path.join(path, \"generator.h5\"))\n",
    "    discriminator.save(os.path.join(path, \"discriminator.h5\"))\n",
    "\n",
    "\n",
    "def main():\n",
    "    # z \\in R^100\n",
    "    latent_dim = 100\n",
    "    # x \\in R^{28x28}\n",
    "    # generator (z -> x)\n",
    "    generator = model_generator()\n",
    "    # discriminator (x -> y)\n",
    "    discriminator = model_discriminator()\n",
    "    example_gan(AdversarialOptimizerSimultaneous(), \"output/gan-cifar10\",\n",
    "                opt_g=Adam(1e-4, decay=1e-5),\n",
    "                opt_d=Adam(1e-3, decay=1e-5),\n",
    "                nb_epoch=100, generator=generator, discriminator=discriminator,\n",
    "                latent_dim=latent_dim)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "# This line allows mpl to run with no DISPLAY defined\n",
    "mpl.use('Agg')\n",
    "\n",
    "from keras.layers import Flatten, Dropout, LeakyReLU, Input, Activation\n",
    "from keras.models import Model\n",
    "from keras.layers.convolutional import UpSampling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets import mnist\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras_adversarial.legacy import Dense, BatchNormalization, Convolution2D\n",
    "from keras_adversarial.image_grid_callback import ImageGridCallback\n",
    "from keras_adversarial import AdversarialModel, simple_gan, gan_targets\n",
    "from keras_adversarial import AdversarialOptimizerSimultaneous, normal_latent_sampling\n",
    "from image_utils import dim_ordering_fix, dim_ordering_input, dim_ordering_reshape, dim_ordering_unfix\n",
    "\n",
    "\n",
    "def leaky_relu(x):\n",
    "    return K.relu(x, 0.2)\n",
    "\n",
    "\n",
    "def model_generator():\n",
    "    nch = 256\n",
    "    g_input = Input(shape=[100])\n",
    "    H = Dense(nch * 14 * 14)(g_input)\n",
    "    H = BatchNormalization(mode=2)(H)\n",
    "    H = Activation('relu')(H)\n",
    "    H = dim_ordering_reshape(nch, 14)(H)\n",
    "    H = UpSampling2D(size=(2, 2))(H)\n",
    "    H = Convolution2D(int(nch / 2), 3, 3, border_mode='same')(H)\n",
    "    H = BatchNormalization(mode=2, axis=1)(H)\n",
    "    H = Activation('relu')(H)\n",
    "    H = Convolution2D(int(nch / 4), 3, 3, border_mode='same')(H)\n",
    "    H = BatchNormalization(mode=2, axis=1)(H)\n",
    "    H = Activation('relu')(H)\n",
    "    H = Convolution2D(1, 1, 1, border_mode='same')(H)\n",
    "    g_V = Activation('sigmoid')(H)\n",
    "    return Model(g_input, g_V)\n",
    "\n",
    "\n",
    "def model_discriminator(input_shape=(1, 28, 28), dropout_rate=0.5):\n",
    "    d_input = dim_ordering_input(input_shape, name=\"input_x\")\n",
    "    nch = 512\n",
    "    # nch = 128\n",
    "    H = Convolution2D(int(nch / 2), 5, 5, subsample=(2, 2), border_mode='same', activation='relu')(d_input)\n",
    "    H = LeakyReLU(0.2)(H)\n",
    "    H = Dropout(dropout_rate)(H)\n",
    "    H = Convolution2D(nch, 5, 5, subsample=(2, 2), border_mode='same', activation='relu')(H)\n",
    "    H = LeakyReLU(0.2)(H)\n",
    "    H = Dropout(dropout_rate)(H)\n",
    "    H = Flatten()(H)\n",
    "    H = Dense(int(nch / 2))(H)\n",
    "    H = LeakyReLU(0.2)(H)\n",
    "    H = Dropout(dropout_rate)(H)\n",
    "    d_V = Dense(1, activation='sigmoid')(H)\n",
    "    return Model(d_input, d_V)\n",
    "\n",
    "\n",
    "def mnist_process(x):\n",
    "    x = x.astype(np.float32) / 255.0\n",
    "    return x\n",
    "\n",
    "\n",
    "def mnist_data():\n",
    "    (xtrain, ytrain), (xtest, ytest) = mnist.load_data()\n",
    "    return mnist_process(xtrain), mnist_process(xtest)\n",
    "\n",
    "\n",
    "def generator_sampler(latent_dim, generator):\n",
    "    def fun():\n",
    "        zsamples = np.random.normal(size=(10 * 10, latent_dim))\n",
    "        gen = dim_ordering_unfix(generator.predict(zsamples))\n",
    "        return gen.reshape((10, 10, 28, 28))\n",
    "\n",
    "    return fun\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # z \\in R^100\n",
    "    latent_dim = 100\n",
    "    # x \\in R^{28x28}\n",
    "    input_shape = (1, 28, 28)\n",
    "\n",
    "    # generator (z -> x)\n",
    "    generator = model_generator()\n",
    "    # discriminator (x -> y)\n",
    "    discriminator = model_discriminator(input_shape=input_shape)\n",
    "    # gan (x - > yfake, yreal), z generated on GPU\n",
    "    gan = simple_gan(generator, discriminator, normal_latent_sampling((latent_dim,)))\n",
    "\n",
    "    # print summary of models\n",
    "    generator.summary()\n",
    "    discriminator.summary()\n",
    "    gan.summary()\n",
    "\n",
    "    # build adversarial model\n",
    "    model = AdversarialModel(base_model=gan,\n",
    "                             player_params=[generator.trainable_weights, discriminator.trainable_weights],\n",
    "                             player_names=[\"generator\", \"discriminator\"])\n",
    "    model.adversarial_compile(adversarial_optimizer=AdversarialOptimizerSimultaneous(),\n",
    "                              player_optimizers=[Adam(1e-4, decay=1e-4), Adam(1e-3, decay=1e-4)],\n",
    "                              loss='binary_crossentropy')\n",
    "\n",
    "    # train model\n",
    "    generator_cb = ImageGridCallback(\"output/gan_convolutional/epoch-{:03d}.png\",\n",
    "                                     generator_sampler(latent_dim, generator))\n",
    "\n",
    "    xtrain, xtest = mnist_data()\n",
    "    xtrain = dim_ordering_fix(xtrain.reshape((-1, 1, 28, 28)))\n",
    "    xtest = dim_ordering_fix(xtest.reshape((-1, 1, 28, 28)))\n",
    "    y = gan_targets(xtrain.shape[0])\n",
    "    ytest = gan_targets(xtest.shape[0])\n",
    "    history = model.fit(x=xtrain, y=y, validation_data=(xtest, ytest), callbacks=[generator_cb], nb_epoch=100,\n",
    "                        batch_size=32)\n",
    "    df = pd.DataFrame(history.history)\n",
    "    df.to_csv(\"output/gan_convolutional/history.csv\")\n",
    "\n",
    "    generator.save(\"output/gan_convolutional/generator.h5\")\n",
    "    discriminator.save(\"output/gan_convolutional/discriminator.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
