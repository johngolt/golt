{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T11:59:31.049303Z",
     "start_time": "2019-01-07T11:59:28.268532Z"
    }
   },
   "source": [
    "## Basic mechanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T02:28:50.769005Z",
     "start_time": "2019-02-22T02:28:48.191599Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T09:31:04.206852Z",
     "start_time": "2019-01-08T09:31:04.093951Z"
    }
   },
   "outputs": [],
   "source": [
    "#tf.enable_eager_execution()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T09:19:27.742853Z",
     "start_time": "2019-01-08T09:19:27.732878Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dtype: 'float32'>\n",
      "(10,)\n",
      "{'a': tf.float32, 'b': tf.int32}\n",
      "{'a': TensorShape([]), 'b': TensorShape([Dimension(100)])}\n",
      "(tf.float32, {'a': tf.float32, 'b': tf.int32})\n",
      "(TensorShape([Dimension(10)]), {'a': TensorShape([]), 'b': TensorShape([Dimension(100)])})\n"
     ]
    }
   ],
   "source": [
    "dataset1 = tf.data.Dataset.from_tensor_slices(tf.random_uniform([4, 10]))\n",
    "print(dataset1.output_types)  # ==> \"tf.float32\"\n",
    "print(dataset1.output_shapes)  # ==> \"(10,)\"\n",
    "\n",
    "dataset2 = tf.data.Dataset.from_tensor_slices(\n",
    "   {\"a\": tf.random_uniform([4]),\n",
    "    \"b\": tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)})\n",
    "print(dataset2.output_types)  # ==> \"{'a': tf.float32, 'b': tf.int32}\"\n",
    "print(dataset2.output_shapes) \n",
    "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
    "print(dataset3.output_types)  # ==> (tf.float32, (tf.float32, tf.int32))\n",
    "print(dataset3.output_shapes) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an iterator\n",
    "#### one-shot\n",
    "the simplest form of iterator, which only supports iterating once through a dataset, with no need for explicit initialization. One-shot iterators handle almost all of the cases that the existing queue-based input pipelines support, but they do not support parameterization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T09:28:09.380515Z",
     "start_time": "2019-01-08T09:28:09.367550Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.range(100)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "for i in range(100):\n",
    "    value =  iterator.get_next()\n",
    "    assert i == value.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initializable\n",
    "run an explicit iterator.initializer operation before using it. it enables you to parameterize the definition of the dataset, using one or more tf.placeholder() tensors that can be fed when you initialize the iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T09:34:52.737563Z",
     "start_time": "2019-01-08T09:34:52.713588Z"
    }
   },
   "outputs": [],
   "source": [
    "max_value = tf.placeholder(tf.int64, shape = [])\n",
    "dataset = tf.data.Dataset.range(max_value)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "sess.run(iterator.initializer, feed_dict = {max_value: 10})\n",
    "for i in range(10):\n",
    "    value = sess.run(next_element)\n",
    "    assert i == value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reinitializable\n",
    "can be initialized from multiple different Dataset objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T09:42:25.183495Z",
     "start_time": "2019-01-08T09:42:24.926430Z"
    }
   },
   "outputs": [],
   "source": [
    "training_dataset = tf.data.Dataset.range(20).map(\n",
    "lambda x: x + tf.random_uniform([], -5, 5, tf.int64))\n",
    "validation_dataset = tf.data.Dataset.range(10)\n",
    "iterator = tf.data.Iterator.from_structure(training_dataset.output_types, \n",
    "                                          training_dataset.output_shapes)\n",
    "next_element = iterator.get_next()\n",
    "training_init_op = iterator.make_initializer(training_dataset)\n",
    "validation_init_op = iterator.make_initializer(validation_dataset)\n",
    "for _ in range(2):\n",
    "    sess.run(training_init_op)\n",
    "    for _ in range(20):\n",
    "        sess.run(next_element)\n",
    "    sess.run(validation_init_op)\n",
    "    for _ in range(10):\n",
    "        sess.run(next_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feedable\n",
    "can be used together with tf.placeholder to select what Iterator to use in each call to tf.Session.run, via the familiar feed_dict mechanism. It offers the same functionality as a reinitializable iterator, but it does not require you to initialize the iterator from the start of a dataset when you switch between iterators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = tf.data.Dataset.range(20).map(\n",
    "lambda x: x + tf.random_uniform([], -5, 5, tf.int64))\n",
    "validation_dataset = tf.data.Dataset.range(10)\n",
    "handle = tf.placeholder(tf.string, shape = [])\n",
    "iterator = tf.data.Iterator.from_string_handle(\n",
    "handle, training_dataset.output_types, training_dataset.output_shapes)\n",
    "next_element = iterator.get_next()\n",
    "training_iterator = training_dataset.make_one_shot_iterator()\n",
    "validation_iterator = validation_dataset.make_initializable_iterator()\n",
    "training_handle = sess.run(training_iterator)\n",
    "validation_handle = sess.run(validation_iterator)\n",
    "while True:\n",
    "    for _ in range(10):\n",
    "        sess.run(next_element, feed_dict={handle:training_handle})\n",
    "    sess.run(validation_iterator.initializer)\n",
    "    for _ in range(5):\n",
    "        sess.run(next_element, feed_dict = {handle: validation_handle})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T10:59:51.744542Z",
     "start_time": "2019-01-08T10:59:51.714591Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(5)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "result = tf.add(next_element, next_element)\n",
    "sess.run(iterator.initializer)\n",
    "while True:\n",
    "    try:\n",
    "        print(sess.run(result))\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving iterator state\n",
    "The `tf.contrib.data.make_saveable_from_iterator` function creates a SaveableObject from an iterator, which can be used to save and restore the current state of the iterator. A saveable object thus created can be added to `tf.train.Saver` variables list or the` tf.GraphKeys.SAVEABLE_OBJECTS` collection for saving and restoring in the same manner as a `tf.Variable`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create saveable object from iterator.\n",
    "saveable = tf.contrib.data.make_saveable_from_iterator(iterator)\n",
    "# Save the iterator state by adding it to the saveable objects collection.\n",
    "tf.add_to_collection(tf.GraphKeys.SAVEABLE_OBJECTS, saveable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading input data\n",
    "### Consuming NumPy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load(\"/var/data/training_data.npy\") as data:\n",
    "    features = data[\"features\"]\n",
    "    labels = data[\"labels\"] \n",
    "# Assume that each row of `features` corresponds to the same row as `labels`.\n",
    "assert features.shape[0] == labels.shape[0]\n",
    "features_placeholder = tf.placeholder(features.dtype, features.shape)\n",
    "labels_placeholder = tf.placeholder(labels.dtype, labels.shape)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features_placeholder, labels_placeholder))\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "sess.run(iterator.initializer, feed_dict={features_placeholder: features,\n",
    "                                          labels_placeholder: labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consuming TFRecord data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = tf.placeholder(tf.string, shape=[None])\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "dataset = dataset.map(...)  # Parse the record into tensors.\n",
    "dataset = dataset.repeat()  # Repeat the input indefinitely.\n",
    "dataset = dataset.batch(32)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "# You can feed the initializer with the appropriate filenames for the current\n",
    "# phase of execution, e.g. training vs. validation.\n",
    "# Initialize `iterator` with training data.\n",
    "training_filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\n",
    "sess.run(iterator.initializer, feed_dict={filenames: training_filenames})\n",
    "# Initialize `iterator` with validation data.\n",
    "validation_filenames = [\"/var/data/validation1.tfrecord\", ...]\n",
    "sess.run(iterator.initializer, feed_dict={filenames: validation_filenames})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consuming text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\"]\n",
    "dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "# Use `Dataset.flat_map()` to transform each file as a separate nested dataset,\n",
    "# and then concatenate their contents sequentially into a single \"flat\" dataset.\n",
    "# * Skip the first line (header row).\n",
    "# * Filter out lines beginning with \"#\" (comments).\n",
    "dataset = dataset.flat_map(\n",
    "    lambda filename: (\n",
    "        tf.data.TextLineDataset(filename)\n",
    "        .skip(1)\n",
    "        .filter(lambda line: tf.not_equal(tf.substr(line, 0, 1), \"#\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consuming CSV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a dataset that reads all of the records from two CSV files, each with\n",
    "# eight float columns\n",
    "filenames = [\"/var/data/file1.csv\", \"/var/data/file2.csv\"]\n",
    "record_defaults = [tf.float32] * 8   # Eight required float columns\n",
    "dataset = tf.contrib.data.CsvDataset(filenames, record_defaults)\n",
    "# Creates a dataset that reads all of the records from two CSV files, each with\n",
    "# four float columns which may have missing values\n",
    "record_defaults = [[0.0]] * 8\n",
    "dataset = tf.contrib.data.CsvDataset(filenames, record_defaults)\n",
    "# Creates a dataset that reads all of the records from two CSV files with\n",
    "# headers, extracting float data from columns 2 and 4.\n",
    "record_defaults = [[0.0]] * 2  # Only provide defaults for the selected columns\n",
    "dataset = tf.contrib.data.CsvDataset(filenames, record_defaults, header=True, select_cols=[2,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data with `Dataset.map()`\n",
    "The `Dataset.map(f)` transformation produces a new dataset by applying a given function f to each element of the input dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads an image from a file, decodes it into a dense tensor, and resizes it\n",
    "# to a fixed shape.\n",
    "def _parse_function(filename, label):\n",
    "    image_string = tf.read_file(filename)\n",
    "    image_decoded = tf.image.decode_jpeg(image_string)\n",
    "    image_resized = tf.image.resize_images(image_decoded, [28, 28])\n",
    "    return image_resized, label\n",
    "# A vector of filenames.\n",
    "filenames = tf.constant([\"/var/data/image1.jpg\", \"/var/data/image2.jpg\", ...])\n",
    "# `labels[i]` is the label for the image in `filenames[i].\n",
    "labels = tf.constant([0, 37, ...])\n",
    "dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "dataset = dataset.map(_parse_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching dataset elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T14:34:10.802401Z",
     "start_time": "2019-01-08T14:34:10.744557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [1 0 0]\n",
      " [2 2 0]\n",
      " [3 3 3]]\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(100)\n",
    "dataset = dataset.map(lambda x: tf.fill([tf.cast(x, tf.int32)], x))\n",
    "dataset = dataset.padded_batch(4, padded_shapes=[None])\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "print(sess.run(next_element))  # ==> [[0, 0, 0], [1, 0, 0], [2, 2, 0], [3, 3, 3]]\n",
    "#print(sess.run(next_element))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training workflows\n",
    "### Processing multiple epochs\n",
    "The simplest way to iterate over a dataset in multiple epochs is to use the `Dataset.repeat()` transformation. \n",
    "### Randomly shuffling input data\n",
    "The `Dataset.shuffle()` transformation randomly shuffles the input dataset using a similar algorithm to tf.RandomShuffleQueue: it maintains a fixed-size buffer and chooses the next element uniformly at random from that buffer.\n",
    "### Using high-level APIs\n",
    "`MonitoredTrainingSession` uses the `tf.errors.OutOfRangeError` to signal that training has completed, so to use it with the tf.data API, we recommend using `Dataset.make_one_shot_iterator()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "dataset = dataset.map(...)\n",
    "dataset = dataset.shuffle(buffer_size=10000)\n",
    "dataset = dataset.batch(32)\n",
    "dataset = dataset.repeat(num_epochs)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "next_example, next_label = iterator.get_next()\n",
    "loss = model_function(next_example, next_label)\n",
    "\n",
    "training_op = tf.train.AdagradOptimizer(...).minimize(loss)\n",
    "\n",
    "with tf.train.MonitoredTrainingSession(...) as sess:\n",
    "    while not sess.should_stop():\n",
    "        sess.run(training_op)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
