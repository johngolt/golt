{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eager executation basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T11:35:43.038982Z",
     "start_time": "2019-01-06T11:35:40.522830Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T11:39:56.514900Z",
     "start_time": "2019-01-06T11:39:56.331658Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor([4 6], shape=(2,), dtype=int32)\n",
      "tf.Tensor(25, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.add(1, 2))\n",
    "print(tf.add([1, 2], [3, 4]))\n",
    "print(tf.square(5))\n",
    "print(tf.reduce_sum([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T11:37:30.551056Z",
     "start_time": "2019-01-06T11:37:30.546069Z"
    }
   },
   "source": [
    "#### Numpy Compatibility\n",
    "- TensorFlow operations automatically convert NumPy ndarrays to Tensors.\n",
    "- NumPy operations automatically convert Tensors to NumPy ndarrays.\n",
    "\n",
    "Tensors can be explicitly converted to NumPy ndarrays by invoking the .numpy() method on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T12:11:41.744834Z",
     "start_time": "2019-01-06T12:11:41.721894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[42. 42. 42.]\n",
      " [42. 42. 42.]\n",
      " [42. 42. 42.]], shape=(3, 3), dtype=float64)\n",
      "[[42. 42. 42.]\n",
      " [42. 42. 42.]\n",
      " [42. 42. 42.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "ndarray = np.ones([3, 3])\n",
    "tensor = tf.multiply(ndarray, 42)\n",
    "print(tensor)\n",
    "print(tensor.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU acceleration\n",
    "The **Tensor.device** property provides a fully qualified string name of the device hosting the contents of the tensor.\n",
    "\n",
    "The string ends with **GPU:N** if the tensor is placed on the N-th GPU on the host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T12:27:08.207371Z",
     "start_time": "2019-01-06T12:27:05.563442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.1 ms ± 2.41 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "def time_matmul(x):\n",
    "    %timeit tf.matmul(x, x)\n",
    "with tf.device('CPU:0'):\n",
    "    x = tf.random_uniform([1000, 1000])\n",
    "    assert x.device.endswith('CPU:0')\n",
    "    time_matmul(x)\n",
    "if tf.test.is_gpu_available():\n",
    "    with tf.device('GPU:0'):\n",
    "        x = tf.random_uniform([1000, 1000])\n",
    "        assert x.device.endswith('GPU:0')\n",
    "        time_matmul(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets\n",
    "This section demonstrates the use of the tf.data.Dataset API to build pipelines to feed data to your model. It covers:\n",
    "- Creating a Dataset.\n",
    "- Iteration over a Dataset with eager execution enabled.\n",
    "##### create a source Dataset\n",
    "Create a source dataset using one of the factory functions like `Dataset.from_tensors`, `Dataset.from_tensor_slices` or using objects that read from files like `TextLineDataset` or `TFRecordDataset`\n",
    "##### Apply transformations\n",
    "Use the transformations functions like `map, batch, shuffle` etc. to apply transformations to the records of the dataset.\n",
    "##### Iterate\n",
    "When eager execution is enabled `Dataset` objects support iteration. If you're familiar with the use of Datasets in TensorFlow graphs, note that there is no need for calls to `Dataset.make_one_shot_iterator()` or `get_next()` calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T12:38:33.749371Z",
     "start_time": "2019-01-06T12:38:33.473831Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 4], shape=(2,), dtype=int32)\n",
      "tf.Tensor([ 9 25], shape=(2,), dtype=int32)\n",
      "tf.Tensor([36 16], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "ds_tensors = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5, 6])\n",
    "ds_tensors = ds_tensors.map(tf.square).shuffle(2).batch(2)\n",
    "for x in ds_tensors:\n",
    "  print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic differentiation and gradient tape\n",
    "TensorFlow provides the tf.GradientTape API for automatic differentiation - computing the gradient of a computation with respect to its input variables. Tensorflow \"records\" all operations executed inside the context of a tf.GradientTape onto a \"tape\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T12:47:56.743695Z",
     "start_time": "2019-01-06T12:47:56.537397Z"
    }
   },
   "outputs": [],
   "source": [
    "x = tf.ones((2, 2))\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(x)\n",
    "    y = tf.reduce_sum(x)\n",
    "    z = tf.multiply(y, y)\n",
    "dz_dx = t.gradient(z, x)\n",
    "for i in [0, 1]:\n",
    "    for j in [ 0, 1]:\n",
    "        assert dz_dx[i][j].numpy() == 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the resources held by a GradientTape are released as soon as GradientTape.gradient() method is called. To compute multiple gradients over the same computation, create a persistent gradient tape. This allows multiple calls to the gradient() method. as resources are released when the tape object is garbage collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T13:27:06.975459Z",
     "start_time": "2019-01-06T13:27:06.968481Z"
    }
   },
   "outputs": [],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape(persistent=True) as t:\n",
    "  t.watch(x)\n",
    "  y = x * x\n",
    "  z = y * y\n",
    "dz_dx = t.gradient(z, x)  # 108.0 (4*x^3 at x = 3)\n",
    "dy_dx = t.gradient(y, x)  # 6.0\n",
    "del t  # Drop the reference to the tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T12:48:30.489995Z",
     "start_time": "2019-01-06T12:48:30.483013Z"
    }
   },
   "source": [
    "#### Higher-order gradients\n",
    "Operations inside of the **GradientTape** context manager are recorded for automatic differentiation. If gradients are computed in that context, then the gradient computation is recorded as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T13:32:40.696408Z",
     "start_time": "2019-01-06T13:32:40.527501Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0 6.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(1.)\n",
    "with tf.GradientTape() as t:\n",
    "    with tf.GradientTape() as t2:\n",
    "        y = x*x*x\n",
    "        dy_dx = t2.gradient(y, x)\n",
    "    d2y_dx2 = t.gradient(dy_dx, x)\n",
    "print(dy_dx.numpy(), d2y_dx2.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom training: Basics\n",
    "Tensors in TensorFlow are immutable stateless objects\n",
    "\n",
    "A Variable is an object which stores a value and, when used in a TensorFlow computation, will implicitly read from this stored value. There are operations (`tf.assign_sub, tf.scatter_update`, etc) which manipulate the value stored in a TensorFlow variable\n",
    "1. Define the model.\n",
    "2. Define a loss function.\n",
    "3. Obtain training data.\n",
    "4. Run through the training data and use an \"optimizer\" to adjust the variables to fit the data.\n",
    "#### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T14:51:52.686387Z",
     "start_time": "2019-01-06T14:51:52.670420Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self):\n",
    "        self.W = tf.Variable(5.0)\n",
    "        self.b = tf.Variable(0.0)   \n",
    "    def __call__(self, x):\n",
    "        return self.W * x + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T15:00:22.047170Z",
     "start_time": "2019-01-06T15:00:22.041187Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss(predicted_y, desired_y):\n",
    "    return tf.reduce_mean(tf.square(predicted_y - desired_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define a training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T15:00:24.693793Z",
     "start_time": "2019-01-06T15:00:24.687844Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, inputs, outputs, learning_rate):\n",
    "    with tf.GradientTape() as t:\n",
    "        current_loss = loss(model(inputs), outputs)\n",
    "    dW, db = t.gradient(current_loss, [model.W, model.b])\n",
    "    model.W.assign_sub(learning_rate * dW)\n",
    "    model.b.assign_sub(learning_rate * db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T15:02:26.852901Z",
     "start_time": "2019-01-06T15:02:26.727241Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.0 0.0 tf.Tensor(8.994057, shape=(), dtype=float32)\n",
      "1 4.5929465 0.39435548 tf.Tensor(6.1051965, shape=(), dtype=float32)\n",
      "2 4.268388 0.7099353 tf.Tensor(4.262118, shape=(), dtype=float32)\n",
      "3 4.009605 0.9624752 tf.Tensor(3.0862277, shape=(), dtype=float32)\n",
      "4 3.8032672 1.1645677 tf.Tensor(2.3359957, shape=(), dtype=float32)\n",
      "5 3.638746 1.3262901 tf.Tensor(1.857332, shape=(), dtype=float32)\n",
      "6 3.5075667 1.4557066 tf.Tensor(1.5519314, shape=(), dtype=float32)\n",
      "7 3.402972 1.5592705 tf.Tensor(1.3570745, shape=(), dtype=float32)\n",
      "8 3.3195744 1.6421462 tf.Tensor(1.2327466, shape=(), dtype=float32)\n",
      "9 3.2530777 1.7084663 tf.Tensor(1.1534189, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "TRUE_W = 3.0\n",
    "TRUE_b = 2.0\n",
    "NUM_EXAMPLES = 1000\n",
    "\n",
    "inputs  = tf.random_normal(shape=[NUM_EXAMPLES])\n",
    "noise   = tf.random_normal(shape=[NUM_EXAMPLES])\n",
    "outputs = inputs * TRUE_W + TRUE_b + noise\n",
    "model = Model()\n",
    "Ws, bs = [], [ ]\n",
    "epochs = range(10)\n",
    "for epoch in epochs:\n",
    "    Ws.append(model.W.numpy())\n",
    "    bs.append(model.b.numpy())\n",
    "    current_loss = loss(model(inputs), outputs)\n",
    "    train(model, inputs, outputs, learning_rate=0.1)\n",
    "    print(epoch, Ws[-1], bs[-1], current_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom layers\n",
    "#### Implementing custom layers\n",
    "The best way to implement your own layer is extending the tf.keras.Layer class and implementing: ` __init__` , where you can do all input-independent initialization ` build`, where you know the shapes of the input tensors and can do the rest of the initialization `call`, where you do the forward computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:35:00.764913Z",
     "start_time": "2019-01-07T05:34:58.156960Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:35:03.959221Z",
     "start_time": "2019-01-07T05:35:03.638673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 1.2422413   0.57676595 -0.7736479   1.4771876   1.9684818   0.95305157\n",
      "   0.78279495  1.1986037   0.8111862  -1.7414144 ]\n",
      " [ 1.2422413   0.57676595 -0.7736479   1.4771876   1.9684818   0.95305157\n",
      "   0.78279495  1.1986037   0.8111862  -1.7414144 ]\n",
      " [ 1.2422413   0.57676595 -0.7736479   1.4771876   1.9684818   0.95305157\n",
      "   0.78279495  1.1986037   0.8111862  -1.7414144 ]], shape=(3, 10), dtype=float32)\n",
      "[<tf.Variable 'my_dense_layer/kernel:0' shape=(5, 10) dtype=float32, numpy=\n",
      "array([[ 0.5884097 , -0.55847716, -0.20097992,  0.05256915,  0.3180158 ,\n",
      "         0.06278121, -0.1324001 ,  0.58853537,  0.18911612, -0.31677678],\n",
      "       [ 0.31581897,  0.01617968,  0.31303602,  0.46660286,  0.15058666,\n",
      "         0.05965292,  0.2888443 ,  0.61008686, -0.16126266, -0.34060073],\n",
      "       [ 0.07550091,  0.5519212 , -0.43838778,  0.630462  ,  0.39130014,\n",
      "        -0.24216768,  0.37751848,  0.16110808,  0.3274299 ,  0.00487834],\n",
      "       [ 0.2899918 ,  0.2021476 , -0.2346243 ,  0.35973012,  0.6052771 ,\n",
      "         0.60907096, -0.2850637 ,  0.20090151,  0.13649642, -0.47415423],\n",
      "       [-0.02748013,  0.36499465, -0.21269187, -0.03217643,  0.50330216,\n",
      "         0.46371418,  0.533896  , -0.362028  ,  0.3194064 , -0.614761  ]],\n",
      "      dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "class MyDenseLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_outputs):\n",
    "        super(MyDenseLayer, self).__init__()\n",
    "        self.num_outputs = num_outputs\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_variable('kernel', \n",
    "                                       shape = [int(input_shape[-1]), \n",
    "                                               self.num_outputs])\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.kernel)\n",
    "layer = MyDenseLayer(10)\n",
    "print(layer(tf.ones([3, 5])))\n",
    "print(layer.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models: composing layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:52:37.183007Z",
     "start_time": "2019-01-07T05:52:37.122173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]]], shape=(1, 2, 3, 3), dtype=float32)\n",
      "['resnet_identity_block_2/conv2d_15/kernel:0', 'resnet_identity_block_2/conv2d_15/bias:0', 'resnet_identity_block_2/batch_normalization_15/gamma:0', 'resnet_identity_block_2/batch_normalization_15/beta:0', 'resnet_identity_block_2/conv2d_16/kernel:0', 'resnet_identity_block_2/conv2d_16/bias:0', 'resnet_identity_block_2/batch_normalization_16/gamma:0', 'resnet_identity_block_2/batch_normalization_16/beta:0', 'resnet_identity_block_2/conv2d_17/kernel:0', 'resnet_identity_block_2/conv2d_17/bias:0', 'resnet_identity_block_2/batch_normalization_17/gamma:0', 'resnet_identity_block_2/batch_normalization_17/beta:0', 'resnet_identity_block_2/batch_normalization_15/moving_mean:0', 'resnet_identity_block_2/batch_normalization_15/moving_variance:0', 'resnet_identity_block_2/batch_normalization_16/moving_mean:0', 'resnet_identity_block_2/batch_normalization_16/moving_variance:0', 'resnet_identity_block_2/batch_normalization_17/moving_mean:0', 'resnet_identity_block_2/batch_normalization_17/moving_variance:0']\n"
     ]
    }
   ],
   "source": [
    "class ResnetIdentityBlock(tf.keras.Model):\n",
    "    def __init__(self, kernel_size, filters):\n",
    "        super(ResnetIdentityBlock, self).__init__(name = '')\n",
    "        filters1, filters2, filters3 = filters\n",
    "        \n",
    "        self.conv2a = tf.keras.layers.Conv2D(filters1, (1, 1))\n",
    "        self.bn2a = tf.keras.layers.BatchNormalization()\n",
    "        self.conv2b = tf.keras.layers.Conv2D(filters2, kernel_size, padding='same')\n",
    "        self.bn2b = tf.keras.layers.BatchNormalization()\n",
    "        self.conv2c = tf.keras.layers.Conv2D(filters3, (1, 1))\n",
    "        self.bn2c = tf.keras.layers.BatchNormalization()\n",
    "    def call(self, input_tensor, training = False):\n",
    "        x = self.conv2a(input_tensor)\n",
    "        x = self.bn2a(x, training = training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.conv2b(x)\n",
    "        x = self.bn2b(x, training = training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.conv2c(x)\n",
    "        x = self.bn2c(x, training = training)\n",
    "        x += input_tensor\n",
    "        return tf.nn.relu(x)\n",
    "\n",
    "block = ResnetIdentityBlock(1, [1, 2, 3])\n",
    "print(block(tf.zeros([1, 2, 3, 3])))\n",
    "print([x.name for x in block.variables])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:51:44.507380Z",
     "start_time": "2019-01-07T05:51:44.166611Z"
    }
   },
   "source": [
    "### Custom Training: walkthrough\n",
    "#### Tensorflow programming\n",
    "This tutorial is structured like many TensorFlow programs:\n",
    "1. Import and parse the data sets.\n",
    "2. Select the type of model.\n",
    "3. Train the model.\n",
    "4. Evaluate the model's effectiveness.\n",
    "5. Use the trained model to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download the dataset\n",
    "##### Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T08:59:11.511949Z",
     "start_time": "2019-01-07T08:59:11.310723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://download.tensorflow.org/data/iris_training.csv\n",
      "\r",
      "8192/2194 [================================================================================================================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "train_dataset_url = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "train_dataset_fp = tf.keras.utils.get_file(fname=os.path.basename(train_dataset_url),\n",
    "                                           origin=train_dataset_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T09:05:00.282428Z",
     "start_time": "2019-01-07T09:05:00.271459Z"
    }
   },
   "outputs": [],
   "source": [
    "column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "feature_names = column_names[:-1]\n",
    "label_name = column_names[-1]\n",
    "class_names = ['Iris setosa', 'Iris versicolor', 'Iris virginica']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create a `tf.data.Dataset`\n",
    "TensorFlow's `Dataset` API handles many common cases for loading data into a model. This is a high-level API for reading data and transforming it into a form used for training.\n",
    "\n",
    "Since the dataset is a CSV-formatted text file, use the `make_csv_dataset` function to parse the data into a suitable format. Since this function generates data for training models, the default behavior is to shuffle the data `(shuffle=True, shuffle_buffer_size=10000)`, and repeat the dataset forever `(num_epochs=None)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T09:06:51.978922Z",
     "start_time": "2019-01-07T09:06:51.929018Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataset = tf.contrib.data.make_csv_dataset(\n",
    "train_dataset_fp, batch_size, column_names = column_names, \n",
    "label_name = label_name, num_epochs = 1)\n",
    "feature, label = next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T09:24:40.638023Z",
     "start_time": "2019-01-07T09:24:40.544986Z"
    }
   },
   "outputs": [],
   "source": [
    "def pack_feature_vector(feature, labels):\n",
    "    features = tf.stack(list(feature.values()), axis = 1)\n",
    "    return features, labels\n",
    "train_dataset = train_dataset.map(pack_feature_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Select the type of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T09:19:13.201094Z",
     "start_time": "2019-01-07T09:19:13.162198Z"
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape = (4, )),\n",
    "    tf.keras.layers.Dense(10, activation = tf.nn.relu), \n",
    "    tf.keras.layers.Dense(3)\n",
    "])\n",
    "# training the model\n",
    "def loss(model, x, y):\n",
    "    y_ = model(x)\n",
    "    return tf.losses.sparse_softmax_cross_entropy(labels = y, logits = y_)\n",
    "def grad(model, inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets)\n",
    "    return loss_value, tape.gradient(loss_value, model.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create an optimizer\n",
    "An optimizer applies the computed gradients to the model's variables to minimize the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T09:25:05.067688Z",
     "start_time": "2019-01-07T09:25:05.039762Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Initial Loss: 1.180802822113037\n",
      "Step: 1,         Loss: 1.1679582595825195\n"
     ]
    }
   ],
   "source": [
    "feature, label = next(iter(train_dataset))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "global_step = tf.Variable(0)\n",
    "loss_value, grads = grad(model, feature, label)\n",
    "print(\"Step: {}, Initial Loss: {}\".format(global_step.numpy(),\n",
    "                                          loss_value.numpy()))\n",
    "optimizer.apply_gradients(zip(grads, model.variables), global_step)\n",
    "print(\"Step: {},         Loss: {}\".format(global_step.numpy(),\n",
    "                                          loss(model, feature, label).numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training loop\n",
    "The following code block sets up these training steps:\n",
    "1. Iterate each epoch. An epoch is one pass through the dataset.\n",
    "2. Within an epoch, iterate over each example in the training Dataset grabbing its features (x) and label (y).\n",
    "3. Using the example's features, make a prediction and compare it with the label. Measure the inaccuracy of the prediction and use that to calculate the model's loss and gradients.\n",
    "4. Use an optimizer to update the model's variables.\n",
    "5. Keep track of some stats for visualization.\n",
    "6. Repeat for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T09:53:00.665334Z",
     "start_time": "2019-01-07T09:53:00.644387Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v1.metrics' has no attribute 'Mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-14c1699b413f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mepoch_loss_avg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mepoch_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow._api.v1.metrics' has no attribute 'Mean'"
     ]
    }
   ],
   "source": [
    "train_loss_results = []\n",
    "train_accuracy_results = [ ]\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss_avg = tf.metrics.Mean()\n",
    "    epoch_accuracy = tf.metrics.Accuracy()\n",
    "    for x, y in train_dataset:\n",
    "        loss_value, grads = grad(model, x, y)\n",
    "        optimizer.apply_gradients(zip(grads, model.variables), \n",
    "                                 global_step)\n",
    "        epoch_loss_avg(loss_value)\n",
    "        epoch_accuracy(tf.argmax(model(x), axis = 1, output_type=tf.int32), y)\n",
    "        train_loss_results.append(epoch_loss_avg.result())\n",
    "        train_accuracy_results.append(epoch_accuracy.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
