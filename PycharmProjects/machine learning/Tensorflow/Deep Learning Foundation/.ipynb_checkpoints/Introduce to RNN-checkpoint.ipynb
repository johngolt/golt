{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 载入所需库和读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T10:44:20.159576Z",
     "start_time": "2019-02-17T10:43:35.006770Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set('abc') #{'a', 'b', 'c'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T10:46:49.874378Z",
     "start_time": "2019-02-17T10:46:49.172251Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:/test/tensorflow/Dataset')\n",
    "with open('anna.txt') as file:\n",
    "    text = file.read()\n",
    "vocab = sorted(set(text))\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype = np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making training mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T12:56:08.892130Z",
     "start_time": "2019-02-17T12:56:08.885150Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, n_steps):\n",
    "    # 对数据进行截取，使得数据满足batch_size * n_steps\n",
    "    chars_per_batch = batch_size * n_steps\n",
    "    n_bathes = len(arr)//chars_per_batch \n",
    "    arr = arr[:n_bathes * chars_per_batch]\n",
    "    # 对数据进行reshape,得到N*(M*K), N:batch size, M:step,K:number of batches \n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    #产生数据的生成器\n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        x = arr[:, n:n+ n_steps]\n",
    "        # 应对last batch label不足的问题\n",
    "        y_temp = arr[:, n+1: n+n_steps + 1]   \n",
    "        y = np.zeros(x.shape, dtype = x.dtype)\n",
    "        y[:, :y_temp.shape[1]] = y_temp\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T12:54:30.032377Z",
     "start_time": "2019-02-17T12:54:29.972568Z"
    }
   },
   "source": [
    "### Building the model\n",
    "#### inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T12:56:10.552694Z",
     "start_time": "2019-02-17T12:56:10.546709Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, num_steps):\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name = 'inputs')\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name = 'targets')\n",
    "    keep_prob = tf.placeholder(tf.float32, name = 'keep')\n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Cell\n",
    "- create a basic LSTM cell with `lstm = tf.contrib.rnn.BasicLSTMCell(num_units)`\n",
    "- add dropout by wrapping it with\n",
    "`tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)`\n",
    "- stack up the LSTM cells into layers with `tf.contrib.rnn.MultiRNNCell`.\n",
    "- create an initial cell state of all zeros. \n",
    "`initial_state = cell.zero_state(batch_size, tf.float32)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T13:01:58.101712Z",
     "start_time": "2019-02-17T13:01:58.093737Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    def build_cell(lstm_size, keep_prob):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        return drop\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN output\n",
    "- output shape: N * M * L L: hidden units 为了方便后来的全连接层reshape成(N * M)* L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T13:06:31.182783Z",
     "start_time": "2019-02-17T13:06:31.173810Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_output(lstm_output, in_size, out_size):\n",
    "    seq_output = tf.concat(lstm_output, axis = 1) # M *(L * N)\n",
    "    x = tf.reshape(seq_output, [-1, in_size]) # (M * N ) * L\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((in_size, out_size), stddev = 0.1))\n",
    "        sofmax_b = tf.Variable(tf.zeros(out_size))\n",
    "        logits = tf.matmul(x, softmax_w) + sofmax_b\n",
    "        out = tf.nn.softmax(logits, name = 'predictions')\n",
    "        return out, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T13:10:25.731847Z",
     "start_time": "2019-02-17T13:10:25.724870Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_class):\n",
    "    y_one_hot = tf.one_hot(targets, num_class)\n",
    "    y_reshaped = tf.reshape(y_one_hot, logits.get_shape())\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits = logits, labels = y_reshaped)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer\n",
    "- Normal RNNs have have issues gradients exploding and disappearing, clip the gradients above some threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T13:10:51.068124Z",
     "start_time": "2019-02-17T13:10:51.061148Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(\n",
    "    loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T13:19:06.440026Z",
     "start_time": "2019-02-17T13:19:06.429057Z"
    }
   },
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    def __init__(self, num_classes, batch_size = 64, \n",
    "                num_steps = 50, lstm_size = 128, num_layers = 2, \n",
    "                learning_rate = 0.001, grad_clip = 5, sampling = False):\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "        tf.reset_default_graph()\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(\n",
    "        batch_size, num_steps)\n",
    "        cell, self.initial_state = build_lstm(lstm_size, \n",
    "                                             num_layers, batch_size, self.keep_prob)\n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state = self.initial_state)\n",
    "        self.final_state = state\n",
    "        self.prediction, self.logits = build_output(\n",
    "        outputs, lstm_size, num_classes)\n",
    "        self.loss = build_loss(self.logits, self.targets, lstm_size, \n",
    "                              num_classes)\n",
    "        self.optimizer = build_optimizer(self.loss, \n",
    "                                        learning_rate, grad_clip)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T12:40:46.999026Z",
     "start_time": "2019-02-17T12:40:46.950153Z"
    }
   },
   "source": [
    "### Time for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T13:26:03.428440Z",
     "start_time": "2019-02-17T13:26:03.422458Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 100        # Sequences per batch\n",
    "num_steps = 100         # Number of sequence steps per batch\n",
    "lstm_size = 512         # Size of hidden layers in LSTMs\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "learning_rate = 0.001   # Learning rate\n",
    "keep_prob = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T12:37:44.499830Z",
     "start_time": "2019-02-17T12:37:44.492851Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 20\n",
    "print_every_n = 50\n",
    "save_every_n = 200\n",
    "model = CharRNN(len(vocab), batch_size = batch_size, \n",
    "               num_steps = num_steps, lstm_size = lstm_size, \n",
    "               num_layers = num_layers, learning_rate = learning_rate)\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            counter += 1\n",
    "            feed = {model.inputs: x, model.targets: y, \n",
    "                   model.keep_prob: keep_prob, model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, model.final_state,\n",
    "                                                 model.optimizer], feed_dict = feed)\n",
    "            if counter%save_every_n ==0:\n",
    "                saver.save(sess, 'checkpoints/i{}_{}.ckpt'.format(\n",
    "                counter, lstm_size))\n",
    "saver.save(sess, 'checkpoints/i{}_l{}'.format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T12:37:47.678336Z",
     "start_time": "2019-02-17T12:37:47.672379Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
