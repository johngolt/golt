{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T06:02:39.930399Z",
     "start_time": "2019-02-17T06:02:38.663536Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'https://raw.githubusercontent.com/udacity/deep-learning/master/sentiment-network/labels.txt'\n",
    "response = requests.get(url)\n",
    "import os\n",
    "os.chdir('C:/test')\n",
    "with open('labels.txt', 'w') as file:\n",
    "    file.write(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T07:34:16.624727Z",
     "start_time": "2019-02-17T07:34:16.250395Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import os\n",
    "os.chdir('C:/test/tensorflow/Dataset')\n",
    "positive = Counter()\n",
    "negative = Counter()\n",
    "total = Counter()\n",
    "g = open('reviews.txt')\n",
    "reviews = list(map(lambda x: x[:-1], g.readlines()))\n",
    "g = open('labels.txt')\n",
    "labels = list(map(lambda x: x[:-1], g.readlines()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T07:38:12.337775Z",
     "start_time": "2019-02-17T07:38:04.300025Z"
    }
   },
   "outputs": [],
   "source": [
    "for i, item in enumerate(reviews):\n",
    "    if labels[i] == 'positive':\n",
    "        for word in item.strip().split(' '):\n",
    "            positive[word] += 1\n",
    "            total[word] += 1\n",
    "    else:\n",
    "        for word in item.strip().split(' '):\n",
    "            negative[word] += 1\n",
    "            total[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T07:43:02.160379Z",
     "start_time": "2019-02-17T07:43:02.027780Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('edie', 4.6913478822291435),\n",
       " ('paulie', 4.07753744390572),\n",
       " ('felix', 3.152736022363656),\n",
       " ('polanski', 2.8233610476132043)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_neg_ratios = Counter()\n",
    "for term, cnt in total.most_common():\n",
    "    if cnt > 100:\n",
    "        pos_neg_ratios[term] = np.log(positive[term]/float(negative[term] + 1))\n",
    "pos_neg_ratios.most_common()[:4]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T07:44:29.380463Z",
     "start_time": "2019-02-17T07:44:29.364504Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74074"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = set(total.keys())\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T07:49:18.360541Z",
     "start_time": "2019-02-17T07:49:18.315623Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "layer0 = np.zeros((1, vocab_size))\n",
    "word2index = { }\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i\n",
    "def update_input_layer(review):\n",
    "    global layer0\n",
    "    layer0 *= 0\n",
    "    for word in review.strip().split(' '):\n",
    "        layer0[0][word2index[word]] += 1\n",
    "update_input_layer(reviews[0])\n",
    "layer0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T08:52:54.713879Z",
     "start_time": "2019-02-17T08:52:54.687949Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "class SentimentNetwork:\n",
    "    def __init__(self, reviews, labels, \n",
    "                hidden_nodes = 10, learning_rate = 0.1):\n",
    "        np.random.seed(1)\n",
    "        self.pre_process_data(reviews, labels)\n",
    "        self.init_network(len(self.review_vocab), hidden_nodes, 1, learning_rate)\n",
    "    \n",
    "    def pre_process_data(self, reviews, labels):\n",
    "        review_vocab = set()\n",
    "        for review in reviews:\n",
    "            words = review.strip().split(' ')\n",
    "            review_vocab.update(words)\n",
    "        self.review_vocab = list(review_vocab)\n",
    "        label_vocab = set()\n",
    "        for label in labels:\n",
    "            label_vocab.add(label)\n",
    "        self.label_vocab = list(label_vocab)\n",
    "        self.word2index = {}\n",
    "        for i, word in enumerate(review_vocab):\n",
    "            self.word2index[word] = i\n",
    "        self.label2index = {}\n",
    "        for i, label in enumerate(label_vocab):\n",
    "            self.label2index[label] = i\n",
    "    \n",
    "    def init_network(self, input_nodes, hidden_nodes, out_nodes, learning_rate):\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.out_nodes = out_nodes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights_0_1 = np.zeros((self.input_nodes, self.hidden_nodes))\n",
    "        self.weights_1_2 = np.random.normal(0.0, self.out_nodes**-0.5, \n",
    "                                                (self.hidden_nodes, self.out_nodes))\n",
    "        self.layer_0 = np.zeros((1, self.input_nodes))\n",
    "    def update_input_layer(self, review):\n",
    "        self.layer_0 *= 0\n",
    "        for word in review.strip().split(' '):\n",
    "            self.layer_0[0][self.word2index.get(word, 0)] += 1\n",
    "    def get_target_for_label(self, label):\n",
    "        if label == 'positive':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "    def sigmoid_output_2_derivative(self, output):\n",
    "        return output * (1 - output)\n",
    "    def train(self, training_reviews, training_labels):\n",
    "        assert len(training_reviews) == len(training_labels) \n",
    "        correct = 0\n",
    "        for review, label in zip(training_reviews, training_labels):\n",
    "            self.update_input_layer(review)\n",
    "            layer_1 = self.layer_0.dot(self.weights_0_1)\n",
    "            layer_2 = self.sigmoid(layer_1.dot(self.weights_1_2))\n",
    "            layer_2_error = layer_2 - self.get_target_for_label(label)\n",
    "            layer_2_delta = layer_2_error * self.sigmoid_output_2_derivative(layer_2)\n",
    "            layer_1_error = layer_2_delta.dot(self.weights_1_2.T)\n",
    "            layer_1_delta = layer_1_error\n",
    "            self.weights_1_2 -= layer_1.T.dot(layer_2_delta) * self.learning_rate\n",
    "            self.weights_0_1 -= self.layer_0.T.dot(layer_1_delta) * self.learning_rate\n",
    "            if (layer_2 >= 0.5) and (label == 'positive'):\n",
    "                correct += 1\n",
    "            elif (layer_2 < 0.5) and (label == 'negative'):\n",
    "                correct += 1\n",
    "    def test(self, testing_reviews, testing_labels):\n",
    "        correct = 0\n",
    "        for review, label in zip(testing_reviews, testing_labels):\n",
    "            pred = self.run(review)\n",
    "            if pred == label:\n",
    "                correct += 1\n",
    "        print(correct, len(testing_reviews), correct/len(testing_reviews))\n",
    "    def run(self, review):\n",
    "        self.update_input_layer(review.lower())\n",
    "        layer_1 = self.layer_0.dot(self.weights_0_1)\n",
    "        layer_2 = self.sigmoid(layer_1.dot(self.weights_1_2))\n",
    "        if layer_2[0] >= 0.5:\n",
    "            return 'positive'\n",
    "        else:\n",
    "            return 'negative'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T08:58:43.714510Z",
     "start_time": "2019-02-17T08:53:02.871726Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "mlp = SentimentNetwork(reviews[:-1000],labels[:-1000], learning_rate=0.1)\n",
    "mlp.train(reviews[:-1000],labels[:-1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T10:03:20.584492Z",
     "start_time": "2019-02-17T10:03:19.139354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 1000 0.5\n"
     ]
    }
   ],
   "source": [
    "mlp.test(reviews[-1000:],labels[-1000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T08:52:37.290095Z",
     "start_time": "2019-02-17T08:52:37.231250Z"
    }
   },
   "source": [
    "### Reduce the Noise in the Input data\n",
    "-         Modify update_input_layer so it does not count how many times each word is used, but rather just stores whether or not a word was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T10:08:09.665754Z",
     "start_time": "2019-02-17T10:08:09.660768Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_input_layer(self,review):\n",
    "        # clear out previous state, reset the layer to be all 0s\n",
    "        self.layer_0 *= 0       \n",
    "        for word in review.split(\" \"):\n",
    "            if(word in self.word2index.keys()):\n",
    "                self.layer_0[0][self.word2index[word]] = 1\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encapsulate our neural network in a class\n",
    "class SentimentNetwork2:\n",
    "    ## New for Project 6: added min_count and polarity_cutoff parameters\n",
    "    def __init__(self, reviews,labels,min_count = 10,polarity_cutoff = 0.1,hidden_nodes = 10, learning_rate = 0.1):\n",
    "        \"\"\"Create a SentimenNetwork with the given settings\n",
    "        Args:\n",
    "            reviews(list) - List of reviews used for training\n",
    "            labels(list) - List of POSITIVE/NEGATIVE labels associated with the given reviews\n",
    "            min_count(int) - Words should only be added to the vocabulary \n",
    "                             if they occur more than this many times\n",
    "            polarity_cutoff(float) - The absolute value of a word's positive-to-negative\n",
    "                                     ratio must be at least this big to be considered.\n",
    "            hidden_nodes(int) - Number of nodes to create in the hidden layer\n",
    "            learning_rate(float) - Learning rate to use while training\n",
    "        \n",
    "        \"\"\"\n",
    "        np.random.seed(1)\n",
    "        self.pre_process_data(reviews, labels, polarity_cutoff, min_count)\n",
    "        self.init_network(len(self.review_vocab),hidden_nodes, 1, learning_rate)\n",
    "\n",
    "    def pre_process_data(self, reviews, labels, polarity_cutoff, min_count):\n",
    "        positive_counts = Counter()\n",
    "        negative_counts = Counter()\n",
    "        total_counts = Counter()\n",
    "\n",
    "        for i in range(len(reviews)):\n",
    "            if(labels[i] == 'POSITIVE'):\n",
    "                for word in reviews[i].split(\" \"):\n",
    "                    positive_counts[word] += 1\n",
    "                    total_counts[word] += 1\n",
    "            else:\n",
    "                for word in reviews[i].split(\" \"):\n",
    "                    negative_counts[word] += 1\n",
    "                    total_counts[word] += 1\n",
    "\n",
    "        pos_neg_ratios = Counter()\n",
    "\n",
    "        for term,cnt in list(total_counts.most_common()):\n",
    "            if(cnt >= 50):\n",
    "                pos_neg_ratio = positive_counts[term] / float(negative_counts[term]+1)\n",
    "                pos_neg_ratios[term] = pos_neg_ratio\n",
    "\n",
    "        for word,ratio in pos_neg_ratios.most_common():\n",
    "            if(ratio > 1):\n",
    "                pos_neg_ratios[word] = np.log(ratio)\n",
    "            else:\n",
    "                pos_neg_ratios[word] = -np.log((1 / (ratio + 0.01)))\n",
    "\n",
    "        review_vocab = set()\n",
    "        for review in reviews:\n",
    "            for word in review.split(\" \"):\n",
    "                if(total_counts[word] > min_count):\n",
    "                    if(word in pos_neg_ratios.keys()):\n",
    "                        if((pos_neg_ratios[word] >= polarity_cutoff) or (pos_neg_ratios[word] <= -polarity_cutoff)):\n",
    "                            review_vocab.add(word)\n",
    "                    else:\n",
    "                        review_vocab.add(word)\n",
    "\n",
    "        # Convert the vocabulary set to a list so we can access words via indices\n",
    "        self.review_vocab = list(review_vocab)\n",
    "        label_vocab = set()\n",
    "        for label in labels:\n",
    "            label_vocab.add(label)\n",
    "\n",
    "        self.label_vocab = list(label_vocab)\n",
    "        \n",
    "        # Store the sizes of the review and label vocabularies.\n",
    "        self.review_vocab_size = len(self.review_vocab)\n",
    "        self.label_vocab_size = len(self.label_vocab)\n",
    "        \n",
    "        # Create a dictionary of words in the vocabulary mapped to index positions\n",
    "        self.word2index = {}\n",
    "        for i, word in enumerate(self.review_vocab):\n",
    "            self.word2index[word] = i\n",
    "        \n",
    "        # Create a dictionary of labels mapped to index positions\n",
    "        self.label2index = {}\n",
    "        for i, label in enumerate(self.label_vocab):\n",
    "            self.label2index[label] = i\n",
    "\n",
    "    def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        # Set number of nodes in input, hidden and output layers.\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "\n",
    "        # Store the learning rate\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Initialize weights\n",
    "\n",
    "        # These are the weights between the input layer and the hidden layer.\n",
    "        self.weights_0_1 = np.zeros((self.input_nodes,self.hidden_nodes))\n",
    "\n",
    "        # These are the weights between the hidden layer and the output layer.\n",
    "        self.weights_1_2 = np.random.normal(0.0, self.output_nodes**-0.5, \n",
    "                                                (self.hidden_nodes, self.output_nodes))\n",
    "        \n",
    "        ## New for Project 5: Removed self.layer_0; added self.layer_1\n",
    "        # The input layer, a two-dimensional matrix with shape 1 x hidden_nodes\n",
    "        self.layer_1 = np.zeros((1,hidden_nodes))\n",
    "    \n",
    "    ## New for Project 5: Removed update_input_layer function\n",
    "    \n",
    "    def get_target_for_label(self,label):\n",
    "        if(label == 'POSITIVE'):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def sigmoid(self,x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_output_2_derivative(self,output):\n",
    "        return output * (1 - output)\n",
    "\n",
    "    def train(self, training_reviews_raw, training_labels):\n",
    "\n",
    "        training_reviews = list()\n",
    "        for review in training_reviews_raw:\n",
    "            indices = set()\n",
    "            for word in review.split(\" \"):\n",
    "                if(word in self.word2index.keys()):\n",
    "                    indices.add(self.word2index[word])\n",
    "            training_reviews.append(list(indices))\n",
    "\n",
    "        # make sure out we have a matching number of reviews and labels\n",
    "        assert(len(training_reviews) == len(training_labels))\n",
    "        correct_so_far = 0\n",
    "        start = time.time()\n",
    "\n",
    "        for i in range(len(training_reviews)):\n",
    "            \n",
    "            # Get the next review and its correct label\n",
    "            review = training_reviews[i]\n",
    "            label = training_labels[i]\n",
    "            self.layer_1 *= 0\n",
    "            for index in review:\n",
    "                self.layer_1 += self.weights_0_1[index]\n",
    "\n",
    "            # Output layer\n",
    "            ## New for Project 5: changed to use 'self.layer_1' instead of 'local layer_1'\n",
    "            layer_2 = self.sigmoid(self.layer_1.dot(self.weights_1_2))            \n",
    "\n",
    "            layer_2_error = layer_2 - self.get_target_for_label(label) # Output layer error is the difference between desired target and actual output.\n",
    "            layer_2_delta = layer_2_error * self.sigmoid_output_2_derivative(layer_2)\n",
    "\n",
    "            # Backpropagated error\n",
    "            layer_1_error = layer_2_delta.dot(self.weights_1_2.T) # errors propagated to the hidden layer\n",
    "            layer_1_delta = layer_1_error # hidden layer gradients - no nonlinearity so it's the same as the error\n",
    "\n",
    "            # Update the weights\n",
    "            ## New for Project 5: changed to use 'self.layer_1' instead of local 'layer_1'\n",
    "            self.weights_1_2 -= self.layer_1.T.dot(layer_2_delta) * self.learning_rate # update hidden-to-output weights with gradient descent step\n",
    "            \n",
    "            ## New for Project 5: Only update the weights that were used in the forward pass\n",
    "            for index in review:\n",
    "                self.weights_0_1[index] -= layer_1_delta[0] * self.learning_rate # update input-to-hidden weights with gradient descent step\n",
    "\n",
    "            # Keep track of correct predictions.\n",
    "            if(layer_2 >= 0.5 and label == 'POSITIVE'):\n",
    "                correct_so_far += 1\n",
    "            elif(layer_2 < 0.5 and label == 'NEGATIVE'):\n",
    "                correct_so_far += 1\n",
    "            \n",
    "            # For debug purposes, print out our prediction accuracy and speed \n",
    "            # throughout the training process. \n",
    "            elapsed_time = float(time.time() - start)\n",
    "            reviews_per_second = i / elapsed_time if elapsed_time > 0 else 0\n",
    "            \n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(training_reviews)))[:4] \\\n",
    "                             + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\\n",
    "                             + \" #Correct:\" + str(correct_so_far) + \" #Trained:\" + str(i+1) \\\n",
    "                             + \" Training Accuracy:\" + str(correct_so_far * 100 / float(i+1))[:4] + \"%\")\n",
    "            if(i % 2500 == 0):\n",
    "                print(\"\")\n",
    "    \n",
    "    def test(self, testing_reviews, testing_labels):\n",
    "        \"\"\"\n",
    "        Attempts to predict the labels for the given testing_reviews,\n",
    "        and uses the test_labels to calculate the accuracy of those predictions.\n",
    "        \"\"\"\n",
    "        \n",
    "        # keep track of how many correct predictions we make\n",
    "        correct = 0\n",
    "\n",
    "        # we'll time how many predictions per second we make\n",
    "        start = time.time()\n",
    "\n",
    "        # Loop through each of the given reviews and call run to predict\n",
    "        # its label. \n",
    "        for i in range(len(testing_reviews)):\n",
    "            pred = self.run(testing_reviews[i])\n",
    "            if(pred == testing_labels[i]):\n",
    "                correct += 1\n",
    "            \n",
    "            # For debug purposes, print out our prediction accuracy and speed \n",
    "            # throughout the prediction process. \n",
    "\n",
    "            elapsed_time = float(time.time() - start)\n",
    "            reviews_per_second = i / elapsed_time if elapsed_time > 0 else 0\n",
    "            \n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(testing_reviews)))[:4] \\\n",
    "                             + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\\n",
    "                             + \" #Correct:\" + str(correct) + \" #Tested:\" + str(i+1) \\\n",
    "                             + \" Testing Accuracy:\" + str(correct * 100 / float(i+1))[:4] + \"%\")\n",
    "    \n",
    "    def run(self, review):\n",
    "        \"\"\"\n",
    "        Returns a POSITIVE or NEGATIVE prediction for the given review.\n",
    "        \"\"\"\n",
    "\n",
    "        self.layer_1 *= 0\n",
    "        unique_indices = set()\n",
    "        for word in review.lower().split(\" \"):\n",
    "            if word in self.word2index.keys():\n",
    "                unique_indices.add(self.word2index[word])\n",
    "        for index in unique_indices:\n",
    "            self.layer_1 += self.weights_0_1[index]\n",
    "        \n",
    "        # Output layer\n",
    "        ## New for Project 5: changed to use self.layer_1 instead of local layer_1\n",
    "        layer_2 = self.sigmoid(self.layer_1.dot(self.weights_1_2))\n",
    "         \n",
    "        # Return POSITIVE for values above greater-than-or-equal-to 0.5 in the output layer;\n",
    "        # return NEGATIVE for other values\n",
    "        if(layer_2[0] >= 0.5):\n",
    "            return \"POSITIVE\"\n",
    "        else:\n",
    "            return \"NEGATIVE\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
