{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入相关库\n",
    "- 得到训练所需的原属数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T10:40:59.853497Z",
     "start_time": "2019-02-18T10:40:59.551271Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.chdir('C:/test/tensorflow/Dataset')\n",
    "with open('reviews.txt') as file:\n",
    "    reviews = file.read()\n",
    "with open('labels.txt') as file:\n",
    "    labels = file.read()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对原始数据进行预处理\n",
    "- 去除数据中的无关的分隔符，得到更加干净的文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T10:41:09.111752Z",
     "start_time": "2019-02-18T10:41:03.048921Z"
    }
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "all_text = ''.join([c for c in reviews if c not in punctuation])\n",
    "reviews = all_text.split('\\n')\n",
    "all_text = ' '.join(reviews)\n",
    "words = all_text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 简历检索字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T10:41:15.912791Z",
     "start_time": "2019-02-18T10:41:13.107284Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key = counts.get, reverse = True)\n",
    "vocab_to_int = {word:i for i, word in enumerate(vocab, 1)} # 用0对数据进行padding，从1开始\n",
    "reviews_ints = []\n",
    "for each in reviews:\n",
    "    reviews_ints.append([vocab_to_int[word] for word in each.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对labels进行处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T10:41:19.534142Z",
     "start_time": "2019-02-18T10:41:19.517149Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = labels.split('\\n')\n",
    "labels = np.array([1 if each =='positive' else 0 for each in labels])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T10:41:23.238205Z",
     "start_time": "2019-02-18T10:41:23.228229Z"
    }
   },
   "source": [
    "### 删除文本中空数据\n",
    "- 删除没用评论的样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T10:45:28.437718Z",
     "start_time": "2019-02-18T10:45:28.415778Z"
    }
   },
   "outputs": [],
   "source": [
    "non_zero_idx = [i for i, review in enumerate(reviews_ints) if len(review) != 0]\n",
    "reviews_ints = [reviews_ints[i] for i in non_zero_idx]\n",
    "labels = np.array([labels[i] for i in non_zero_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对数据进行padding\n",
    "- 对少于200的字符进行0padding, 对长于200进行截取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T10:49:32.041045Z",
     "start_time": "2019-02-18T10:49:31.552860Z"
    }
   },
   "outputs": [],
   "source": [
    "seq_len = 200\n",
    "features = np.zeros((len(reviews_ints), seq_len), dtype = np.int32)\n",
    "for i, row in enumerate(reviews_ints):\n",
    "    features[i, -len(row):] = np.array(row)[:seq_len]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对数据进行切分\n",
    "- Training, Validation, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T11:21:08.305442Z",
     "start_time": "2019-02-18T11:21:08.298462Z"
    }
   },
   "outputs": [],
   "source": [
    "split_frac = 0.8\n",
    "split_idx = int(len(features) * split_frac)\n",
    "train_x, val_x = features[:split_idx], features[split_idx:]\n",
    "train_y, val_y = labels[:split_idx], labels[split_idx:]\n",
    "test_idx = int(len(val_x) * 0.5)\n",
    "val_x, test_x= val_x[:test_idx], val_x[test_idx:]\n",
    "val_y, test_y = val_y[:test_idx], val_y[test_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Graph\n",
    "### 定义超参数和定义Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T12:00:52.471138Z",
     "start_time": "2019-02-18T12:00:52.460203Z"
    }
   },
   "outputs": [],
   "source": [
    "lstm_size = 256\n",
    "lstm_layers = 1\n",
    "batch_size = 500\n",
    "learning_rate = 0.001\n",
    "n_words = len(vocab_to_int) + 1\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(tf.int32, [None, None], name = 'inputs')\n",
    "    labels_ = tf.placeholder(tf.int32, [None, None], name = 'labels')\n",
    "    keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T12:00:56.642322Z",
     "start_time": "2019-02-18T12:00:56.621379Z"
    }
   },
   "outputs": [],
   "source": [
    "embed_size = 300\n",
    "with graph.as_default():\n",
    "    embedding = tf.Variable(tf.random_uniform((n_words, embed_size), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs_)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob) # dropout\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)# 初始状态"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Forward Pass\n",
    "- `outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, initial_state=initial_state)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T12:08:14.476620Z",
     "start_time": "2019-02-18T12:08:14.338988Z"
    }
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, \n",
    "                                            embed, initial_state = initial_state)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output\n",
    "- grab the last output with `outputs[:, -1]`, the calculate the cost from that and labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T12:08:19.217246Z",
     "start_time": "2019-02-18T12:08:18.746270Z"
    }
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    predictions = tf.contrib.layers.fully_connected(\n",
    "outputs[:, -1], 1, activation_fn = tf.sigmoid)\n",
    "    cost = tf.losses.mean_squared_error(labels_, predictions)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate= learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T12:08:23.331922Z",
     "start_time": "2019-02-18T12:08:23.319026Z"
    }
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    correct_pred = tf.equal(tf.cast(\n",
    "    tf.round(predictions), tf.int32), labels_)\n",
    "    accuracy = tf.reduce_mean(tf.cast(\n",
    "    correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T12:13:08.035164Z",
     "start_time": "2019-02-18T12:13:08.029180Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_batches(x, y ,batch_size = batch_size):\n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches * batch_size], y[:n_batches * batch_size]\n",
    "    for i in range(0, len(x), batch_size):\n",
    "        yield x[i:i+batch_size], y[i:i + batch_size]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-18T12:13:12.028Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0/10 Iteration:5 Train loss:0.245\n",
      "Epoch:0/10 Iteration:10 Train loss:0.238\n",
      "Epoch:0/10 Iteration:15 Train loss:0.220\n",
      "Epoch:0/10 Iteration:20 Train loss:0.189\n",
      "Epoch:0/10 Iteration:25 Train loss:0.221\n",
      "Val acc: 0.695\n",
      "Epoch:0/10 Iteration:30 Train loss:0.205\n",
      "Epoch:0/10 Iteration:35 Train loss:0.182\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "with tf.Session(graph = graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    for e in range(epochs):\n",
    "        state = sess.run(initial_state)\n",
    "        for i, (x, y) in enumerate(get_batches(train_x, train_y)):\n",
    "            feed = {inputs_:x, \n",
    "                   labels_: y[:, None], \n",
    "                   keep_prob: 0.5, \n",
    "                   initial_state: state}\n",
    "            loss, state, _ = sess.run([cost, final_state, optimizer], feed_dict = feed)\n",
    "            if iteration % 5 ==0:\n",
    "                print('Epoch:{}/{}'.format(e, epochs), \n",
    "                     'Iteration:{}'.format(iteration), \n",
    "                     'Train loss:{:.3f}'.format(loss))\n",
    "            if iteration % 25 == 0:\n",
    "                val_acc = []\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                for x, y in get_batches(val_x, val_y, batch_size):\n",
    "                    feed = {inputs_:x, \n",
    "                           labels_: y[:, None], \n",
    "                           keep_prob:1, \n",
    "                           initial_state: val_state}\n",
    "                    batch_acc, val_state = sess.run([accuracy, final_state], feed_dict = feed)\n",
    "                    val_acc.append(batch_acc)\n",
    "                print('Val acc: {:.3f}'.format(np.mean(val_acc)))\n",
    "            iteration += 1\n",
    "        saver.save(sess, 'checkpoints/setiment.ckpt')            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T12:07:51.524063Z",
     "start_time": "2019-02-18T12:07:51.517049Z"
    }
   },
   "outputs": [],
   "source": [
    "test_acc = []\n",
    "with tf.Session(graph = graph) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    for i, (x, y) in enumerate(get_batches(test_x, test_y, batch_size)):\n",
    "        feed = {inputs_:x, \n",
    "               labels_: y, \n",
    "               keep_prob: 1, \n",
    "               initial_state: test_state}\n",
    "        batch_acc, test_state = sess.run([accuracy, final_state], feed_dict = feed)\n",
    "        test_acc.append(batch_acc)\n",
    "    print('Test accuracy: {:.3f}'.format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T12:28:38.619146Z",
     "start_time": "2019-02-18T12:28:38.614159Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T12:29:02.345238Z",
     "start_time": "2019-02-18T12:29:02.341252Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T12:29:39.713834Z",
     "start_time": "2019-02-18T12:29:39.707821Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
