{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "# 网格搜索可以作为pipeline中的元素，从而进行搜索。\n",
    "def Imbalance(estimator, grid_params, X, y):\n",
    "    grid = GridSearchCV(estimator=estimator,grid_params=grid_params,\n",
    "                       n_jobs=-1,cv=3)\n",
    "    for train, test in KFold(5).split(X, y):\n",
    "        pipeline=imbalanced_make_pipeline(\n",
    "        SMOTE(sampling_strategy='minority'), grid)\n",
    "        model = pipeline.fit(X[train], y[train])\n",
    "        best = grid.best_estimator_\n",
    "        prediction = best.predict(X[test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of small samples from the original dataset is based on the intuition that one class characteristics (non fraud) will differ from that of the other (fraud). To distinguish these characteristics we need to show the autoencoders only one class of data. This is because the autoencoder will try to learn only one class and automaticlly distinuish the other class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "\n",
    "from keras.layers import Input,Dense\n",
    "import keras.regularizers as regularizers\n",
    "from keras.models import Model\n",
    "import sklearn.preprocessing as preprocessing\n",
    "\n",
    "input_layer = Input(shape=(X.shape[1],))\n",
    "encoded = Dense(100, activation='tanh', \n",
    "                activity_regularizer=regularizers.l1(10e-3))(input_layer)\n",
    "encoded = Dense(50, activation='relu')(encoded)\n",
    "\n",
    "decoded = Dense(50, activation='tanh')(encoded)\n",
    "decoded = Dense(100, activation='tanh')(decoded)\n",
    "output_layer = Dense(X.shape[1], activation='relu')(decoded)\n",
    "autoencoder = Model(input_layer, output_layer)\n",
    "autoencoder.compile(optimizer=\"adadelta\", loss=\"mse\")\n",
    "\n",
    "x = data.drop([\"Class\"], axis=1)\n",
    "y = data[\"Class\"].values\n",
    "\n",
    "x_scale = preprocessing.MinMaxScaler().fit_transform(x.values)\n",
    "x_norm, x_fraud = x_scale[y == 0], x_scale[y == 1]\n",
    "autoencoder.fit(x_norm[0:2000], x_norm[0:2000], \n",
    "                batch_size = 256, epochs = 10, \n",
    "                shuffle = True, validation_split = 0.20);\n",
    "\n",
    "'''Now, the model is trained. We are intereseted in obtaining \n",
    "latent representation of the input learned by the model.\n",
    "This can be accessed by the weights of the trained model. \n",
    "We will create another network containing sequential layers, \n",
    "and we will only add the trained\n",
    "weights till the third layer where latent representation exists. '''\n",
    "from keras.models import Sequential\n",
    "hidden_representation = Sequential()\n",
    "hidden_representation.add(autoencoder.layers[0])\n",
    "hidden_representation.add(autoencoder.layers[1])\n",
    "hidden_representation.add(autoencoder.layers[2])\n",
    "\n",
    "'''Generate the hidden representations of two classes : \n",
    "non-fraud and fraud by predicting the raw inputs using the above model.'''\n",
    "norm_hid_rep = hidden_representation.predict(x_norm[:3000])\n",
    "fraud_hid_rep = hidden_representation.predict(x_fraud)\n",
    "\n",
    "\n",
    "rep_x = np.append(norm_hid_rep, fraud_hid_rep, axis = 0)\n",
    "y_n = np.zeros(norm_hid_rep.shape[0])\n",
    "y_f = np.ones(fraud_hid_rep.shape[0])\n",
    "rep_y = np.append(y_n, y_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate majority and minority classes\n",
    "df_majority = df_train[df_train.def_pay==0]\n",
    "df_minority = df_train[df_train.def_pay==1]\n",
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority,replace=True,   \n",
    "                    n_samples=18677, random_state=587) # reproducible results\n",
    "# Combine majority class with upsampled minority class\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "# Downsample majority class\n",
    "df_majority_downsampled = resample(df_majority, replace=False,   \n",
    "                 n_samples=5323, random_state=587) # reproducible results\n",
    "# Combine minority class with downsampled majority class\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm = SMOTE(random_state=589, ratio = 1.0)\n",
    "X_SMOTE, y_SMOTE = sm.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = np.random.randn(1000,6)\n",
    "df = pd.DataFrame(df,columns=list('ABCDEF'))\n",
    "\n",
    "df.loc[:,:] = np.where(df>0,0,1)\n",
    "\n",
    "df.columns\n",
    "\n",
    "fig = plt.figure(figsize=(8,10))\n",
    "ax = fig.add_subplot()\n",
    "for i, feature in enumerate(df.columns):\n",
    "    x = np.array([i]*df.shape[0])\n",
    "    y = np.arange(df.shape[0])\n",
    "    mask=df[feature]==0\n",
    "    ax.scatter(x[~mask],y[~mask],c='k',s=1)\n",
    "ax.xaxis.set_ticks(range(len(df.columns)))\n",
    "ax.xaxis.set_ticklabels(df.columns);\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "def VIF(data):\n",
    "    data = data.to_numpy()\n",
    "    res = [variance_inflation_factor(data, i) for i in range(data.shape[1])]\n",
    "    s,v,d = np.linalg.svd(data)\n",
    "    ratio = v.max()/v.min()\n",
    "    return res, ratio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
