{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T06:32:34.532611Z",
     "start_time": "2019-08-26T06:31:52.702438Z"
    }
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier,LGBMRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {'boosting_type': 'gbdt', 'objective': 'binary', \n",
    "          'learning_rate': 0.1, \n",
    "    'num_leaves': 50, 'max_depth': 6,'subsample': 0.8, \n",
    "          'colsample_bytree': 0.8}\n",
    "\n",
    "'''为了确定估计器的数目，也就是boosting迭代的次数，也可以说是残差树的数目，\n",
    "参数名为n_estimators/num_iterations/num_round/num_boost_round。\n",
    "我们可以先将该参数设成一个较大的数，然后在cv结果中查看最优的迭代次数，'''\n",
    "\n",
    "data_train = lgb.Dataset(df_train, y_train, silent=True)\n",
    "cv_results = lgb.cv(params, data_train, num_boost_round=1000, nfold=5, \n",
    "                    stratified=False, shuffle=True, metrics='rmse',\n",
    "    early_stopping_rounds=50, verbose_eval=50, show_stdv=True, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 我们可以创建lgb的sklearn模型，使用上面选择的(学习率，评估器数目)\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=50,\n",
    "                              learning_rate=0.1, n_estimators=43, max_depth=6,\n",
    "                              metric='rmse', bagging_fraction = 0.8,feature_fraction = 0.8)\n",
    "\n",
    "params_test1={'max_depth': range(3,8,2),\n",
    "    'num_leaves':range(50, 170, 30)}\n",
    "'''sklearn模型评估里的scoring参数都是采用的higher return values are \n",
    "better than lower return values（较高的返回值优于较低的返回值）。\n",
    "但是，我采用的metric策略采用的是均方误差(rmse)，越低越好，所以sklearn就提供了neg_mean_squared_erro参数，\n",
    "也就是返回metric的负数，所以就均方差来说，也就变成负数越大越好了。'''\n",
    "gsearch1 = GridSearchCV(estimator=model_lgb, param_grid=params_test1, \n",
    "                        scoring='neg_mean_squared_error', cv=5, verbose=1, n_jobs=4)\n",
    "gsearch1.fit(df_train, y_train)\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_\n",
    "#先粗调再细调\n",
    "params_test2={'max_depth': [6,7,8],\n",
    "    'num_leaves':[68,74,80,86,92]}\n",
    "gsearch2 = GridSearchCV(estimator=model_lgb, param_grid=params_test2, \n",
    "                        scoring='neg_mean_squared_error', cv=5, verbose=1, n_jobs=4)\n",
    "gsearch2.fit(df_train, y_train)\n",
    "gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#细调参数\n",
    "params_test3={'min_child_samples': [18, 19, 20, 21, 22], 'min_child_weight':[0.001, 0.002]}\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=80,\n",
    "                              learning_rate=0.1, n_estimators=43, max_depth=7, \n",
    "                              metric='rmse', bagging_fraction = 0.8, feature_fraction = 0.8)\n",
    "gsearch3 = GridSearchCV(estimator=model_lgb, param_grid=params_test3, \n",
    "                        scoring='neg_mean_squared_error', cv=5, verbose=1, n_jobs=4)\n",
    "gsearch3.fit(df_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#调节样本选取参数和特征选取参数\n",
    "params_test4={'feature_fraction': [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "              'bagging_fraction': [0.6, 0.7, 0.8, 0.9, 1.0]}\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=80,\n",
    "                              learning_rate=0.1, n_estimators=43, max_depth=7, \n",
    "                              metric='rmse', bagging_freq = 5,  min_child_samples=20)\n",
    "gsearch4 = GridSearchCV(estimator=model_lgb, param_grid=params_test4, \n",
    "                        scoring='neg_mean_squared_error', cv=5, verbose=1, n_jobs=4)\n",
    "gsearch4.fit(df_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#细调特征选取参数\n",
    "params_test5={'feature_fraction': [0.62, 0.65, 0.68, 0.7, 0.72, 0.75, 0.78 ]}\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=80,\n",
    "                              learning_rate=0.1, n_estimators=43, max_depth=7, \n",
    "                              metric='rmse',  min_child_samples=20)\n",
    "gsearch5 = GridSearchCV(estimator=model_lgb, param_grid=params_test5, \n",
    "                        scoring='neg_mean_squared_error', cv=5, verbose=1, n_jobs=4)\n",
    "gsearch5.fit(df_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#调节正则化系数\n",
    "params_test6={'reg_alpha': [0, 0.001, 0.01, 0.03, 0.08, 0.3, 0.5],\n",
    "    'reg_lambda': [0, 0.001, 0.01, 0.03, 0.08, 0.3, 0.5]}\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=80,\n",
    "                              learning_rate=0.b1, n_estimators=43, max_depth=7, \n",
    "                              metric='rmse',  min_child_samples=20, feature_fraction=0.7)\n",
    "gsearch6 = GridSearchCV(estimator=model_lgb, param_grid=params_test6, \n",
    "                        scoring='neg_mean_squared_error', cv=5, verbose=1, n_jobs=4)\n",
    "gsearch6.fit(df_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'boosting_type': 'gbdt', 'objective': 'regression', 'learning_rate': 0.005, \n",
    "    'num_leaves': 80, 'max_depth': 7,'min_data_in_leaf': 20,\n",
    "          'subsample': 1, 'colsample_bytree': 0.7}\n",
    "\n",
    "data_train = lgb.Dataset(df_train, y_train, silent=True)\n",
    "cv_results = lgb.cv(\n",
    "    params, data_train, num_boost_round=10000, nfold=5, stratified=False, \n",
    "    shuffle=True, metrics='rmse',\n",
    "    early_stopping_rounds=50, verbose_eval=100, show_stdv=True)\n",
    "\n",
    "print('best n_estimators:', len(cv_results['rmse-mean']))\n",
    "print('best cv score:', cv_results['rmse-mean'][-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
