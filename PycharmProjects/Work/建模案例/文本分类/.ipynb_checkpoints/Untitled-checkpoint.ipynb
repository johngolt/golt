{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T08:32:06.474967Z",
     "start_time": "2020-08-17T08:32:06.470976Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"D:/DataSet/自然语言\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T08:32:16.292827Z",
     "start_time": "2020-08-17T08:32:15.577601Z"
    }
   },
   "source": [
    "文本表示方法\n",
    "- one-hot 将每一个单词使用一个离散的向量表示，将每个字/词编码成一个索引，然后根据索引进行赋值。\n",
    "- bags of words 每个文档的字/词可以使用其出现次数来进行表示\n",
    "- N-gram 与Count Vectors类似，不过加入了相邻单词组合为新的单词，并进行计数。\n",
    "- tf-idf 第一部分是词语频率，第二部分是逆文档频率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "train_df = pd.read_csv('./data/train_set.csv', sep='\\t', nrows=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count Vectors + RidgeClassifier\n",
    "vectorizer = CountVectorizer(max_features=3000)\n",
    "train_test = vectorizer.fit_transform(train_df['text'])\n",
    "\n",
    "clf = RidgeClassifier()\n",
    "clf.fit(train_test[:10000], train_df['label'].values[:10000])\n",
    "\n",
    "val_pred = clf.predict(train_test[10000:])\n",
    "print(f1_score(train_df['label'].values[10000:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF + RidgeClassifier\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,3), max_features=3000)\n",
    "train_test = tfidf.fit_transform(train_df['text'])\n",
    "\n",
    "clf = RidgeClassifier()\n",
    "clf.fit(train_test[:10000], train_df['label'].values[:10000])\n",
    "\n",
    "val_pred = clf.predict(train_test[10000:])\n",
    "print(f1_score(train_df['label'].values[10000:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#正则化参数对模型的影响\n",
    "sample = train_df[0:5000]\n",
    "n = int(2*len(sample)/3)\n",
    "tfidf = TfidfVectorizer(ngram_range=(2,3), max_features=2500)\n",
    "train_test = tfidf.fit_transform(sample['text'])\n",
    "train_x = train_test[:n]\n",
    "train_y = sample['label'].values[:n]\n",
    "test_x = train_test[n:]\n",
    "test_y = sample['label'].values[n:]\n",
    "\n",
    "f1 = []\n",
    "for i in range(10):\n",
    "    clf = RidgeClassifier(alpha = 0.15*(i+1), solver = 'sag')\n",
    "    clf.fit(train_x, train_y)\n",
    "    val_pred = clf.predict(test_x)\n",
    "    f1.append(f1_score(test_y, val_pred, average='macro'))\n",
    "\n",
    "plt.plot([0.15*(i+1) for i in range(10)], f1)\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('f1_score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_features对模型的影响\n",
    "f1 = []\n",
    "features = [1000,2000,3000,4000]\n",
    "for i in range(4):\n",
    "    tfidf = TfidfVectorizer(ngram_range=(2,3), max_features=features[i])\n",
    "    train_test = tfidf.fit_transform(sample['text'])\n",
    "    train_x = train_test[:n]\n",
    "    train_y = sample['label'].values[:n]\n",
    "    test_x = train_test[n:]\n",
    "    test_y = sample['label'].values[n:]\n",
    "    clf = RidgeClassifier(alpha = 0.1*(i+1), solver = 'sag')\n",
    "    clf.fit(train_x, train_y)\n",
    "    val_pred = clf.predict(test_x)\n",
    "    f1.append(f1_score(test_y, val_pred, average='macro'))\n",
    "\n",
    "plt.plot(features, f1)\n",
    "plt.xlabel('max_features')\n",
    "plt.ylabel('f1_score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngram_range对模型的影响\n",
    "#n-gram提取词语字符数的下边界和上边界，考虑到中文的用词习惯，ngram_range可以在(1,4)之间选取\n",
    "f1 = []\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,1), max_features=2000)\n",
    "train_test = tfidf.fit_transform(sample['text'])\n",
    "train_x = train_test[:n]\n",
    "train_y = sample['label'].values[:n]\n",
    "test_x = train_test[n:]\n",
    "test_y = sample['label'].values[n:]\n",
    "clf = RidgeClassifier(alpha = 0.1*(i+1), solver = 'sag')\n",
    "clf.fit(train_x, train_y)\n",
    "val_pred = clf.predict(test_x)\n",
    "f1.append(f1_score(test_y, val_pred, average='macro'))\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(2,2), max_features=2000)\n",
    "train_test = tfidf.fit_transform(sample['text'])\n",
    "train_x = train_test[:n]\n",
    "train_y = sample['label'].values[:n]\n",
    "test_x = train_test[n:]\n",
    "test_y = sample['label'].values[n:]\n",
    "clf = RidgeClassifier(alpha = 0.1*(i+1), solver = 'sag')\n",
    "clf.fit(train_x, train_y)\n",
    "val_pred = clf.predict(test_x)\n",
    "f1.append(f1_score(test_y, val_pred, average='macro'))\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(3,3), max_features=2000)\n",
    "train_test = tfidf.fit_transform(sample['text'])\n",
    "train_x = train_test[:n]\n",
    "train_y = sample['label'].values[:n]\n",
    "test_x = train_test[n:]\n",
    "test_y = sample['label'].values[n:]\n",
    "clf = RidgeClassifier(alpha = 0.1*(i+1), solver = 'sag')\n",
    "clf.fit(train_x, train_y)\n",
    "val_pred = clf.predict(test_x)\n",
    "f1.append(f1_score(test_y, val_pred, average='macro'))\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,3), max_features=2000)\n",
    "train_test = tfidf.fit_transform(sample['text'])\n",
    "train_x = train_test[:n]\n",
    "train_y = sample['label'].values[:n]\n",
    "test_x = train_test[n:]\n",
    "test_y = sample['label'].values[n:]\n",
    "clf = RidgeClassifier(alpha = 0.1*(i+1), solver = 'sag')\n",
    "clf.fit(train_x, train_y)\n",
    "val_pred = clf.predict(test_x)\n",
    "f1.append(f1_score(test_y, val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,3), max_features=5000)\n",
    "train_test = tfidf.fit_transform(train_df['text']) # 词向量 15000*max_features\n",
    "\n",
    "reg = linear_model.LogisticRegression(penalty='l2', C=1.0,solver='liblinear')\n",
    "reg.fit(train_test[:10000], train_df['label'].values[:10000])\n",
    "\n",
    "val_pred = reg.predict(train_test[10000:])\n",
    "print('预测结果中各类新闻数目')\n",
    "print(pd.Series(val_pred).value_counts())\n",
    "print('\\n F1 score为')\n",
    "print(f1_score(train_df['label'].values[10000:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,3), max_features=5000)\n",
    "train_test = tfidf.fit_transform(train_df['text']) # 词向量 15000*max_features\n",
    "\n",
    "reg = linear_model.SGDClassifier(loss=\"log\", penalty='l2', alpha=0.0001,l1_ratio=0.15)\n",
    "reg.fit(train_test[:10000], train_df['label'].values[:10000])\n",
    "\n",
    "val_pred = reg.predict(train_test[10000:])\n",
    "print('预测结果中各类新闻数目')\n",
    "print(pd.Series(val_pred).value_counts())\n",
    "print('\\n F1 score为')\n",
    "print(f1_score(train_df['label'].values[10000:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,3), max_features=5000)\n",
    "train_test = tfidf.fit_transform(train_df['text']) # 词向量 15000*max_features\n",
    "\n",
    "reg = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto',decision_function_shape='ovr')\n",
    "reg.fit(train_test[:10000], train_df['label'].values[:10000])\n",
    "\n",
    "val_pred = reg.predict(train_test[10000:])\n",
    "print('预测结果中各类新闻数目')\n",
    "print(pd.Series(val_pred).value_counts())\n",
    "print('\\n F1 score为')\n",
    "print(f1_score(train_df['label'].values[10000:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型搭建遵循以下步骤：\n",
    "添加输入层（embedding层）。Embedding层的输入是一批文档，每个文档由一个词汇索引序列构成。例如：[10, 30, 80, 1000] 可能表示“我 昨天 来到 达观数据”这个短文本，其中“我”、“昨天”、“来到”、“达观数据”在词汇表中的索引分别是10、30、80、1000；Embedding层将每个单词映射成EMBEDDING_DIM维的向量。于是：input_shape=(BATCH_SIZE, MAX_WORDS), output_shape=(BATCH_SIZE,MAX_WORDS, EMBEDDING_DIM)；\n",
    "添加隐含层（投影层）。投影层对一个文档中所有单词的向量进行叠加平均。keras提供的GlobalAveragePooling1D类可以帮我们实现这个功能。这层的input_shape是Embedding层的output_shape，这层的output_shape=( BATCH_SIZE, EMBEDDING_DIM)；\n",
    "添加输出层（softmax层）。真实的fastText这层是Hierarchical Softmax，因为keras原生并没有支持Hierarchical Softmax，所以这里用Softmax代替。这层指定了CLASS_NUM，对于一篇文档，输出层会产生CLASS_NUM个概率值，分别表示此文档属于当前类的可能性。这层的output_shape=(BATCH_SIZE, CLASS_NUM)\n",
    "指定损失函数、优化器类型、评价指标，编译模型。损失函数我们设置为categorical_crossentropy，它就是我们上面所说的softmax回归的损失函数；优化器我们设置为SGD，表示随机梯度下降优化器；评价指标选择accuracy，表示精度。\n",
    "\n",
    "用训练数据feed模型时，你需要：\n",
    "将文档分好词，构建词汇表。词汇表中每个词用一个整数（索引）来代替，并预留“未知词”索引，假设为0；\n",
    "对类标进行onehot化。假设我们文本数据总共有3个类别，对应的类标分别是1、2、3，那么这三个类标对应的onehot向量分别是[1, 0,0]、[0, 1, 0]、[0, 0, 1]；\n",
    "对一批文本，将每个文本转化为词索引序列，每个类标转化为onehot向量。就像之前的例子，“我 昨天 来到 达观数据”可能被转化为[10, 30, 80, 1000]；它属于类别1，它的类标就是[1, 0, 0]。由于我们设置了MAX_WORDS=500，这个短文本向量后面就需要补496个0，即[10, 30, 80, 1000, 0, 0, 0, …, 0]。因此，batch_xs的 维度为( BATCH_SIZE,MAX_WORDS)，batch_ys的维度为（BATCH_SIZE, CLASS_NUM）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.layers import Dense\n",
    "\n",
    "VOCAB_SIZE = 2000\n",
    "EMBEDDING_DIM = 100\n",
    "MAX_WORDS = 500\n",
    "CLASS_NUM = 5\n",
    "\n",
    "\n",
    "def build_fastText():\n",
    "    model = Sequential()\n",
    "    # 将词汇数VOCAB_SIZE映射为EMBEDDING_DIM维\n",
    "    model.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_WORDS))\n",
    "    # 平均文档中所有词的embedding\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    # softmax分类\n",
    "    model.add(Dense(CLASS_NUM, activation='softmax'))\n",
    "    # 定义损失函数、优化器、分类度量指标\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_fastText()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主要超参数：\n",
    "lr: 学习率\n",
    "dim: 词向量的维度\n",
    "epoch: 每轮的个数\n",
    "wordNgrams: 词的n-gram，一般设置为2或3\n",
    "loss: 损失函数 ns(negative sampling, 负采样)、hs(hierarchical softmax, 分层softmax)、softmax、ova(One-VS-ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import fasttext\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def fasttext_model(nrows, train_num, lr=1.0, wordNgrams=2, minCount=1, epoch=25, loss='hs', dim=100):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 转换为FastText需要的格式\n",
    "    train_df = pd.read_csv('../input/train_set.csv', \n",
    "                           sep='\\t', nrows=nrows)\n",
    "\n",
    "    # shuffle\n",
    "    train_df = shuffle(train_df, random_state=666)\n",
    "\n",
    "    train_df['label_ft'] = '__label__' + train_df['label'].astype('str')\n",
    "    train_df[['text', 'label_ft']].iloc[:train_num].to_csv(\n",
    "        '../input/fastText_train.csv', index=None, header=None, sep='\\t')\n",
    "\n",
    "    model = fasttext.train_supervised(\n",
    "        '../input/fastText_train.csv', lr=lr, wordNgrams=wordNgrams,\n",
    "        verbose=2, minCount=minCount, epoch=epoch, loss=loss, dim=dim)\n",
    "\n",
    "    train_pred = [model.predict(x)[0][0].split('__')[-1] for x in train_df.iloc[:train_num]['text']]\n",
    "    print('Train f1_score:', f1_score(train_df['label'].values[:train_num].astype(str), train_pred, average='macro'))\n",
    "    val_pred = [model.predict(x)[0][0].split('__')[-1] for x in train_df.iloc[train_num:]['text']]\n",
    "    print('Val f1_score:', f1_score(train_df['label'].values[train_num:].astype(str), val_pred, average='macro'))\n",
    "    train_time = time.time()\n",
    "    print('Train time: {:.2f}s'.format(train_time - start_time))\n",
    "\n",
    "     # 预测并保存\n",
    "    test_df = pd.read_csv('../input/test_a.csv')\n",
    "\n",
    "    test_pred = [model.predict(x)[0][0].split('__')[-1] for x in test_df['text']]\n",
    "    test_pred = pd.DataFrame(test_pred, columns=['label'])\n",
    "    test_pred.to_csv('../input/test_fastText_ridgeclassifier.csv', index=False)\n",
    "    print('Test predict saved.')\n",
    "    end_time = time.time()\n",
    "    print('Predict time:{:.2f}s'.format(end_time - train_time))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':  \n",
    "    nrows = 200000\n",
    "    train_num = int(nrows * 0.7)\n",
    "    lr=0.01\n",
    "    wordNgrams=2\n",
    "    minCount=1\n",
    "    epoch=25\n",
    "    loss='hs'\n",
    "\n",
    "    fasttext_model(nrows, train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasttext_kfold_model(nrows, train_num, n_splits, lr=1.0, wordNgrams=2, minCount=1, epoch=25, loss='hs', dim=100):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 转换为FastText需要的格式\n",
    "    train_df = pd.read_csv('../input/train_set.csv', sep='\\t', nrows=nrows)\n",
    "\n",
    "    # shuffle\n",
    "    train_df = shuffle(train_df, random_state=666)\n",
    "\n",
    "    train_df['label_ft'] = '__label__' + train_df['label'].astype('str')\n",
    "\n",
    "    models = []\n",
    "    train_scores = []\n",
    "    val_scores = []\n",
    "\n",
    "    # K折交叉验证\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=666)\n",
    "    for train_index, test_index in skf.split(train_df['text'], train_df['label_ft']):\n",
    "        train_df[['text', 'label_ft']].iloc[train_index].to_csv('../input/fastText_train.csv', index=None, header=None, sep='\\t')\n",
    "\n",
    "        model = fasttext.train_supervised('../input/fastText_train.csv', lr=lr, wordNgrams=wordNgrams, verbose=2, \n",
    "                                          minCount=minCount, epoch=epoch, loss=loss)\n",
    "        models.append(model)\n",
    "\n",
    "        train_pred = [model.predict(x)[0][0].split('__')[-1] for x in train_df.iloc[train_index]['text']]\n",
    "        train_score = f1_score(train_df['label'].values[train_index].astype(str), train_pred, average='macro')\n",
    "        # print('Train length: ', len(train_pred))\n",
    "        print('Train score: ', train_score)\n",
    "        train_scores.append(train_score)\n",
    "\n",
    "        val_pred = [model.predict(x)[0][0].split('__')[-1] for x in train_df.iloc[test_index]['text']]\n",
    "        val_score = f1_score(train_df['label'].values[test_index].astype(str), val_pred, average='macro')\n",
    "        # print('Val length: ', len(val_pred))\n",
    "        print('Val score', val_score)\n",
    "        val_scores.append(val_score)\n",
    "\n",
    "    print('mean train score: ', np.mean(train_scores))\n",
    "    print('mean val score: ', np.mean(val_scores))\n",
    "    train_time = time.time()\n",
    "    print('Train time: {:.2f}s'.format(train_time - start_time))\n",
    "\n",
    "    return models\n",
    "\n",
    "def fasttext_kfold_predict(models, n_splits):\n",
    "\n",
    "    pred_list = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    # 预测并保存\n",
    "    test_df = pd.read_csv('../input/test_a.csv')\n",
    "\n",
    "    # 消耗时间较长\n",
    "    for model in models:\n",
    "        test_pred = [model.predict(x)[0][0].split('__')[-1] for x in test_df['text']]\n",
    "        pred_list.append(test_pred)\n",
    "\n",
    "    test_pred_label = pd.DataFrame(pred_list).T.apply(lambda row: np.argmax(np.bincount([row[i] for i in range(n_splits)])), axis=1)\n",
    "    test_pred_label.columns='label'\n",
    "\n",
    "    test_pred_label.to_csv('../input/test_fastText_ridgeclassifier.csv', index=False)\n",
    "    print('Test predict saved.')\n",
    "    end_time = time.time()\n",
    "    print('Predict time:{:.2f}s'.format(end_time - start_time))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    nrows = 200000\n",
    "    train_num = int(nrows * 0.7)\n",
    "    n_splits = 3\n",
    "    lr=0.1\n",
    "    wordNgrams=2\n",
    "    minCount=1\n",
    "    epoch=25\n",
    "    loss='hs'\n",
    "    dim=200\n",
    "\n",
    "    \"\"\"\n",
    "    Train score:  0.9635013320936988\n",
    "    Val score 0.9086640111428032\n",
    "    Train score:  0.9623510782430645\n",
    "    Val score 0.9094998879044359\n",
    "    Train score:  0.9628121318772955\n",
    "    Val score 0.9096191534698315\n",
    "    mean train score:  0.9628881807380196\n",
    "    mean val score:  0.9092610175056901\n",
    "    Train time: 740.60s\n",
    "    \"\"\"   \n",
    "\n",
    "    models = fasttext_kfold_model(nrows, train_num, n_splits, lr=lr, wordNgrams=wordNgrams, minCount=minCount, epoch=epoch, loss=loss, dim=dim)\n",
    "    fasttext_kfold_predict(models, n_splits=n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
