{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "551ce207-0976-48c3-9242-fac3e6bdf527",
    "_uuid": "66406036d8dd7a0071295d1aee64f13bffc44e3a"
   },
   "source": [
    "There are 7 different sources of data:\n",
    "\n",
    "* application_train/application_test: the main training and testing data with information about each loan application at Home Credit. Every loan has its own row and is identified by the feature `SK_ID_CURR`. The training application data comes with the `TARGET` indicating 0: the loan was repaid or 1: the loan was not repaid. \n",
    "* bureau: data concerning client's previous credits from other financial institutions. Each previous credit has its own row in bureau, but one loan in the application data can have multiple previous credits.\n",
    "* bureau_balance: monthly data about the previous credits in bureau. Each row is one month of a previous credit, and a single previous credit can have multiple rows, one for each month of the credit length. \n",
    "* previous_application: previous applications for loans at Home Credit of clients who have loans in the application data. Each current loan in the application data can have multiple previous loans. Each previous application has one row and is identified by the feature `SK_ID_PREV`. \n",
    "* POS_CASH_BALANCE: monthly data about previous point of sale or cash loans clients have had with Home Credit. Each row is one month of a previous point of sale or cash loan, and a single previous loan can have many rows.\n",
    "* credit_card_balance: monthly data about previous credit cards clients have had with Home Credit. Each row is one month of a credit card balance, and a single credit card can have many rows.\n",
    "* installments_payment: payment history for previous loans at Home Credit. There is one row for every made payment and one row for every missed payment. \n",
    "\n",
    "This diagram shows how all of the data is related:\n",
    "\n",
    "![image](https://storage.googleapis.com/kaggle-media/competitions/home-credit/home_credit.png)\n",
    "\n",
    "Moreover, we are provided with the definitions of all the columns (in `HomeCredit_columns_description.csv`) and an example of the expected submission file. \n",
    "\n",
    "In this notebook, we will stick to using only the main application training and testing data. Although if we want to have any hope of seriously competing, we need to use all the data, for now we will stick to one file which should be more manageable. This will let us establish a baseline that we can then improve upon. With these projects, it's best to build up an understanding of the problem a little at a time rather than diving all the way in and getting completely lost! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d632b08c-d252-4238-b496-e2c6edebec4b",
    "_uuid": "eb13bf76d4e1e60d0703856ec391cdc2c5bdf1fb"
   },
   "source": [
    "## Imports\n",
    "\n",
    "We are using a typical data science stack: `numpy`, `pandas`, `sklearn`, `matplotlib`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a5e67831-4751-4f11-8e07-527e3e092671",
    "_uuid": "ded520f73b9e94ed47ac2e994a5fb1bcb9093d0f"
   },
   "source": [
    "## Read in Data \n",
    "\n",
    "First, we can list all the available data files. There are a total of 9 files: 1 main file for training (with target) 1 main file for testing (without the target), 1 example submission file, and 6 other files containing additional information about each loan. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2cdca894-e637-43a9-8f80-5791c2bb9041",
    "_uuid": "c54e1559611512ebd447ac24f2226c2fffd61dcd"
   },
   "outputs": [],
   "source": [
    "# List files available\n",
    "print(os.listdir(\"../input/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Training data\n",
    "app_train = pd.read_csv('../input/application_train.csv')\n",
    "print('Training data shape: ', app_train.shape)\n",
    "app_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4695541966d3d29e8a7a8975b072d01caff1631d"
   },
   "source": [
    "The training data has 307511 observations (each one a separate loan) and 122 features (variables) including the `TARGET` (the label we want to predict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d077aee0-5271-440e-bc07-6087eab40b74",
    "_uuid": "cbd1c4111df6f07bc0d479b51f50895e728b717a"
   },
   "outputs": [],
   "source": [
    "# Testing data features\n",
    "app_test = pd.read_csv('../input/application_test.csv')\n",
    "print('Testing data shape: ', app_test.shape)\n",
    "app_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e351f02c8a5886756507a2d4f1ddba4791220f12"
   },
   "source": [
    "The test set is considerably smaller and lacks a `TARGET` column. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5fb6ab16-1b38-4ecf-8123-e48c7c061773",
    "_uuid": "2163ca09678b53dbe88388ccbc7d0e0f7d6c6230"
   },
   "outputs": [],
   "source": [
    "app_train['TARGET'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0e93c1e2-f6b8-4a0b-82b6-7dad8df56048",
    "_uuid": "1b2611fb3cf392023c3f40fd2f7b96f56f5dee7d"
   },
   "outputs": [],
   "source": [
    "app_train['TARGET'].astype(int).plot.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "48f008ff-d81e-46b2-80a3-e58f2a6627ca",
    "_uuid": "119106000875202a0030109f14b73245fc4285e1"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "507ec6b1-99d0-4324-a3ed-bdea2f916227",
    "_uuid": "58851dfef481f32b3026e89b086534ea3683440d"
   },
   "source": [
    "## Examine Missing Values\n",
    "\n",
    "Next we can look at the number and percentage of missing values in each column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fc4c675f-e4a1-4e4f-9ece-3c59e5c8f7fd",
    "_uuid": "7a2f5c72c45fa04d9fa95e8051ae595be806e9a2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to calculate missing values by column# Funct \n",
    "def missing_values_table(df):\n",
    "        # Total missing values\n",
    "        mis_val = df.isnull().sum()\n",
    "        \n",
    "        # Percentage of missing values\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        \n",
    "        # Make a table with the results\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        \n",
    "        # Rename the columns\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        \n",
    "        # Sort the table by percentage of missing descending\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        \n",
    "        # Print some summary information\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        \n",
    "        # Return the dataframe with missing information\n",
    "        return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "786881f0-235e-441c-8319-f715a3b7d920",
    "_uuid": "98b0a82a3009b8f6d0bc718a2e1eaba779b4ace9"
   },
   "outputs": [],
   "source": [
    "# Missing values statistics\n",
    "missing_values = missing_values_table(app_train)\n",
    "missing_values.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "df3c6a1d-b3ff-4565-bb32-5cba43c52729",
    "_uuid": "0b1be19103910ce83ebf54eeed99e42829643578"
   },
   "source": [
    "Another option would be to drop columns with a high percentage of missing values, although it is impossible to know ahead of time if these columns will be helpful to our model. Therefore, we will keep all of the columns for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0672e40c3ab75a7901c0de35d248b322a227dc7f"
   },
   "source": [
    "## Column Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a03caadd76fa32f4b193e52467d4f39f2145d7b6"
   },
   "outputs": [],
   "source": [
    "# Number of each type of column\n",
    "app_train.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5859303c9acc63f7ff7acce063a9cd022a6d38cd"
   },
   "source": [
    "Let's now look at the number of unique entries in each of the `object` (categorical) columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2d021eda10939a19b141292d34491b357acd201a"
   },
   "outputs": [],
   "source": [
    "# Number of unique classes in each object column\n",
    "app_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "10ceaf3ba31e26c822b242b1278d93ebfbefcc0a"
   },
   "source": [
    "Most of the categorical variables have a relatively small number of unique entries. We will need to find a way to deal with these categorical variables! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "86d1b309-5524-4298-b873-2c1c09eddec6",
    "_uuid": "1b49e667293daabffd8a4b2b6d02cf44bf6a3ba8"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "95627792-157e-457a-88a8-3b3875c7e1d5",
    "_uuid": "46f5bf9a6de52e270aa911ffd895e704da5426ec"
   },
   "source": [
    "### Label Encoding and One-Hot Encoding\n",
    "\n",
    "Let's implement the policy described above: for any categorical variable (`dtype == object`) with 2 unique categories, we will use label encoding, and for any categorical variable with more than 2 unique categories, we will use one-hot encoding. \n",
    "\n",
    "For label encoding, we use the Scikit-Learn `LabelEncoder` and for one-hot encoding, the pandas `get_dummies(df)` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "70641d4d-1075-4837-8972-e58d70d8f242",
    "_uuid": "ddfaae5c3dcc7ec6bb47a2dffc10d364e8d25355"
   },
   "outputs": [],
   "source": [
    "# Create a label encoder object\n",
    "le = LabelEncoder()\n",
    "le_count = 0\n",
    "\n",
    "# Iterate through the columns\n",
    "for col in app_train:\n",
    "    if app_train[col].dtype == 'object':\n",
    "        # If 2 or fewer unique categories\n",
    "        if len(list(app_train[col].unique())) <= 2:\n",
    "            # Train on the training data\n",
    "            le.fit(app_train[col])\n",
    "            # Transform both training and testing data\n",
    "            app_train[col] = le.transform(app_train[col])\n",
    "            app_test[col] = le.transform(app_test[col])\n",
    "            \n",
    "            # Keep track of how many columns were label encoded\n",
    "            le_count += 1\n",
    "            \n",
    "print('%d columns were label encoded.' % le_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0851773b-39fd-4cf0-9a66-e30adeef3e57",
    "_uuid": "6796c6dc793a08e162b6e20c6f185ef37bdf51f3"
   },
   "outputs": [],
   "source": [
    "app_train = pd.get_dummies(app_train)\n",
    "app_test = pd.get_dummies(app_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "61d910b5-84f5-4655-bd8a-d29672c13741",
    "_uuid": "1b2c4198638ec8e5155097d112249de8754eb5c0"
   },
   "source": [
    "### Aligning Training and Testing Data\n",
    "\n",
    "There need to be the same features (columns) in both the training and testing data. One-hot encoding has created more columns in the training data because there were some categorical variables with categories not represented in the testing data. To remove the columns in the training data that are not in the testing data, we need to `align` the dataframes. First we extract the target column from the training data (because this is not in the testing data but we need to keep this information). When we do the align, we must make sure to set `axis = 1` to align the dataframes based on the columns and not on the rows!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d99ca215-e893-490c-a6a4-83f3e8a067b3",
    "_uuid": "e0d12a13cb95521c19b10d8829e8abe2b1118396"
   },
   "outputs": [],
   "source": [
    "train_labels = app_train['TARGET']\n",
    "\n",
    "# Align the training and testing data, keep only columns present in both dataframes\n",
    "app_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\n",
    "\n",
    "# Add the target back in\n",
    "app_train['TARGET'] = train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "802ffdbae02e43a9ca4e256ffcd6bd40ae15f3e9"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "13918211-0e6b-4d72-955b-f997db19eea2",
    "_uuid": "4d7c8dd1d5bb5a0ef84cb78e6bff927249e62145"
   },
   "source": [
    "## Back to Exploratory Data Analysis\n",
    "\n",
    "### Anomalies\n",
    "\n",
    "One problem we always want to be on the lookout for when doing EDA is anomalies within the data. These may be due to mis-typed numbers, errors in measuring equipment, or they could be valid but extreme measurements. One way to support anomalies quantitatively is by looking at the statistics of a column using the `describe` method. The numbers in the `DAYS_BIRTH` column are negative because they are recorded relative to the current loan application. To see these stats in years, we can mutliple by -1 and divide by the number of days in a year:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a60be93c2d7d63855e6d65c1109f408ad85da134"
   },
   "outputs": [],
   "source": [
    "(app_train['DAYS_BIRTH'] / -365).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "acb37a3e3f2e0b2fd581259788b9255398314157"
   },
   "source": [
    "Those ages look reasonable. There are no outliers for the age on either the high or low end. How about the days of employment? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "600c59dd5d970d3ccfea3a6af0036d85958adc91"
   },
   "outputs": [],
   "source": [
    "app_train['DAYS_EMPLOYED'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1cdd9dafce28e497e08062cd3b189ac353c04cd9"
   },
   "source": [
    "That doesn't look right! The maximum value (besides being positive) is about 1000 years! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2878bfb3a2be4554f33e03e1a04d4c1978b52a08"
   },
   "outputs": [],
   "source": [
    "app_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\n",
    "plt.xlabel('Days Employment');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d28ca1e799c0a6113cc5e920297e1dc93d380af4"
   },
   "source": [
    "Just out of curiousity, let's subset the anomalous clients and see if they tend to have higher or low rates of default than the rest of the clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "67ea87d9ef6974b1780a7db1eefd13f90f81b5be"
   },
   "outputs": [],
   "source": [
    "anom = app_train[app_train['DAYS_EMPLOYED'] == 365243]\n",
    "non_anom = app_train[app_train['DAYS_EMPLOYED'] != 365243]\n",
    "print('The non-anomalies default on %0.2f%% of loans' % (\n",
    "    100 * non_anom['TARGET'].mean()))\n",
    "print('The anomalies default on %0.2f%% of loans' % (\n",
    "    100 * anom['TARGET'].mean()))\n",
    "print('There are %d anomalous days of employment' % len(anom))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1edfcf786aadb004f083e9896989a29e43bf80da"
   },
   "source": [
    "Well that is extremely interesting! It turns out that the anomalies have a lower rate of default. \n",
    "\n",
    "Handling the anomalies depends on the exact situation, with no set rules. One of the safest approaches is just to set the anomalies to a missing value and then have them filled in (using Imputation) before machine learning. In this case, since all the anomalies have the exact same value, we want to fill them in with the same value in case all of these loans share something in common. The anomalous values seem to have some importance, so we want to tell the machine learning model if we did in fact fill in these values. As a solution, we will fill in the anomalous values with not a number (`np.nan`) and then create a new boolean column indicating whether or not the value was anomalous.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e23ec3cb89428f3dd994b572f718cc729740cfab"
   },
   "outputs": [],
   "source": [
    "# Create an anomalous flag column\n",
    "app_train['DAYS_EMPLOYED_ANOM'] = app_train[\"DAYS_EMPLOYED\"] == 365243\n",
    "\n",
    "# Replace the anomalous values with nan\n",
    "app_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n",
    "\n",
    "app_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\n",
    "plt.xlabel('Days Employment');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "839595437c0721e2480f6b4ee58f3060b222f166"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a0d7c77b2adecaa878f39cf86ffddcfbbe51a190"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fd656b392faad3b34ecfa448b55ad03e75449e0a"
   },
   "source": [
    "### Correlations\n",
    "\n",
    "Now that we have dealt with the categorical variables and the outliers, let's continue with the EDA. One way to try and understand the data is by looking for correlations between the features and the target. We can calculate the Pearson correlation coefficient between every variable and the target using the `.corr` dataframe method.\n",
    "\n",
    "The correlation coefficient is not the greatest method to represent \"relevance\" of a feature, but it does give us an idea of possible relationships within the data. Some [general interpretations of the absolute value of the correlation coefficent](http://www.statstutor.ac.uk/resources/uploaded/pearsons.pdf) are:\n",
    "\n",
    "\n",
    "* .00-.19 “very weak”\n",
    "*  .20-.39 “weak”\n",
    "*  .40-.59 “moderate”\n",
    "*  .60-.79 “strong”\n",
    "* .80-1.0 “very strong”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "02acdb8d-d95f-41b9-8ad1-e2b6cb26f398",
    "_uuid": "d39d15d64db1f2c9015c6f542911ef9a9cac119e"
   },
   "outputs": [],
   "source": [
    "# Find correlations with the target and sort\n",
    "correlations = app_train.corr()['TARGET'].sort_values()\n",
    "\n",
    "# Display correlations\n",
    "print('Most Positive Correlations:\\n', correlations.tail(15))\n",
    "print('\\nMost Negative Correlations:\\n', correlations.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8cfa409c-ec74-4fa4-8093-e7d00596c9c5",
    "_uuid": "67e1f0f22ec8e26c38827c24ca1e9409d73c9c64"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0f7b1cfb-9e5c-4720-9618-ad326940f3f3",
    "_uuid": "c1b831b6d1c3221efb123fbc1a4882aa1f598ec0"
   },
   "source": [
    "### Effect of Age on Repayment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b0ab583c-dfbb-4ff7-80e5-d747fc408499",
    "_uuid": "f705c7aa49486ec3bf119c4edc4e4af58861b88d"
   },
   "outputs": [],
   "source": [
    "# Find the correlation of the positive days since birth and target\n",
    "app_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])\n",
    "app_train['DAYS_BIRTH'].corr(app_train['TARGET'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3fde277c-ebf1-4eaf-a353-c18fc4b518a6",
    "_uuid": "2b95e2c33bdd50682e7105d0f27b9cc3ad5b482d"
   },
   "source": [
    "As the client gets older, there is a negative linear relationship with the target meaning that as clients get older, they tend to repay their loans on time more often. \n",
    "\n",
    "Let's start looking at this variable. First, we can make a histogram of the age. We will put the x axis in years to make the plot a little more understandable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "35e36393-e388-488e-ba7a-7473169d3e6f",
    "_uuid": "739226c4594130d6aabeb25ffb8742c37657d7a4"
   },
   "outputs": [],
   "source": [
    "# Set the style of plots\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Plot the distribution of ages in years\n",
    "plt.hist(app_train['DAYS_BIRTH'] / 365, edgecolor = 'k', bins = 25)\n",
    "plt.title('Age of Client'); plt.xlabel('Age (years)'); plt.ylabel('Count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "02f5d3c5-e527-430b-a38d-531aeb8f3dd1",
    "_uuid": "340680b4a4ecf310a6369808157b17cac7c13461"
   },
   "source": [
    "By itself, the distribution of age does not tell us much other than that there are no outliers as all the ages are reasonable. To visualize the effect of the age on the target, we will next make a [kernel density estimation plot](https://en.wikipedia.org/wiki/Kernel_density_estimation) (KDE) colored by the value of the target. A [kernel density estimate plot shows the distribution of a single variable](https://chemicalstatistician.wordpress.com/2013/06/09/exploratory-data-analysis-kernel-density-estimation-in-r-on-ozone-pollution-data-in-new-york-and-ozonopolis/) and can be thought of as a smoothed histogram (it is created by computing a kernel, usually a Gaussian, at each data point and then averaging all the individual kernels to develop a single smooth curve). We will use the seaborn `kdeplot` for this graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3982a18f-2731-4bb2-80c9-831b2377421f",
    "_uuid": "2e045e65f048789b577477356df4337c9e5e2087"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 8))\n",
    "\n",
    "# KDE plot of loans that were repaid on time\n",
    "sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'DAYS_BIRTH'] / 365, label = 'target == 0')\n",
    "\n",
    "# KDE plot of loans which were not repaid on time\n",
    "sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'DAYS_BIRTH'] / 365, label = 'target == 1')\n",
    "\n",
    "# Labeling of plot\n",
    "plt.xlabel('Age (years)'); plt.ylabel('Density'); plt.title('Distribution of Ages');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9749e164-efea-47d2-ab60-5a8b89ff0570",
    "_uuid": "57757e02285b8067b61e3f586174ad64bec78ac1"
   },
   "source": [
    "The target == 1 curve skews towards the younger end of the range. Although this is not a significant correlation (-0.07 correlation coefficient), this variable is likely going to be useful in a machine learning model because it does affect the target. Let's look at this relationship in another way: average failure to repay loans by age bracket. \n",
    "\n",
    "To make this graph, first we `cut` the age category into bins of 5 years each. Then, for each bin, we calculate the average value of the target, which tells us the ratio of loans that were not repaid in each age category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4296e926-7245-40df-bb0a-f6e59d8e566a",
    "_uuid": "6c50572f095bff250bfed1993e2c53118277b5dd"
   },
   "outputs": [],
   "source": [
    "# Age information into a separate dataframe\n",
    "age_data = app_train[['TARGET', 'DAYS_BIRTH']]\n",
    "age_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] / 365\n",
    "\n",
    "# Bin the age data\n",
    "age_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins = np.linspace(20, 70, num = 11))\n",
    "age_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "18873d6b-3877-4c77-830e-0f3e10e5e7fb",
    "_uuid": "7082483e5fd9114856926de28968e5ae0b478b36"
   },
   "outputs": [],
   "source": [
    "# Group by the bin and calculate averages\n",
    "age_groups  = age_data.groupby('YEARS_BINNED').mean()\n",
    "age_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "004d1021-d73f-4356-9ef8-0464c95d1708",
    "_uuid": "823b5032f472b05ce079ae5a7680389f31ddd8b7"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 8))\n",
    "\n",
    "# Graph the age bins and the average of the target as a bar plot\n",
    "plt.bar(age_groups.index.astype(str), 100 * age_groups['TARGET'])\n",
    "\n",
    "# Plot labeling\n",
    "plt.xticks(rotation = 75); plt.xlabel('Age Group (years)'); plt.ylabel('Failure to Repay (%)')\n",
    "plt.title('Failure to Repay by Age Group');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2dad060f-bcab-4fe3-aa19-29fbf3e6fdab",
    "_uuid": "eb2bd6392ed6d6f7e002bc8dbea6aab0f30487d9"
   },
   "source": [
    "There is a clear trend: younger applicants are more likely to not repay the loan! The rate of failure to repay is above 10% for the youngest three age groups and beolow 5% for the oldest age group.\n",
    "\n",
    "This is information that could be directly used by the bank: because younger clients are less likely to repay the loan, maybe they should be provided with more guidance or financial planning tips. This does not mean the bank should discriminate against younger clients, but it would be smart to take precautionary measures to help younger clients pay on time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4749204f-ec63-4eeb-8d25-9c80967348f1",
    "_uuid": "43a3bb87bdaa65509e9dc887492239ae06cd1c77"
   },
   "source": [
    "### Exterior Sources\n",
    "\n",
    "The 3 variables with the strongest negative correlations with the target are `EXT_SOURCE_1`, `EXT_SOURCE_2`, and `EXT_SOURCE_3`.\n",
    "According to the documentation, these features represent a \"normalized score from external data source\". I'm not sure what this exactly means, but it may be a cumulative sort of credit rating made using numerous sources of data. \n",
    "\n",
    "Let's take a look at these variables.\n",
    "\n",
    "First, we can show the correlations of the `EXT_SOURCE` features with the target and with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e2ab3b7f-3a53-4495-a1de-31ad287f032a",
    "_uuid": "6197819149feaff75176e64e54c65ea6be3864fe"
   },
   "outputs": [],
   "source": [
    "# Extract the EXT_SOURCE variables and show correlations\n",
    "ext_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n",
    "ext_data_corrs = ext_data.corr()\n",
    "ext_data_corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0479863d-cfa9-47ab-83e6-7d7877e3e939",
    "_uuid": "20b21a6b4e15a726c29596abeb01346dc416729c"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 6))\n",
    "\n",
    "# Heatmap of correlations\n",
    "sns.heatmap(ext_data_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\n",
    "plt.title('Correlation Heatmap');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "78bd5acc-003d-4795-a57a-a6c4fc9c8c5f",
    "_uuid": "6a592aa7c01858b268489ccb8fd00690cd26cd58"
   },
   "source": [
    "All three `EXT_SOURCE` featureshave negative correlations with the target, indicating that as the value of the `EXT_SOURCE` increases, the client is more likely to repay the loan. We can also see that `DAYS_BIRTH` is positively correlated with `EXT_SOURCE_1` indicating that maybe one of the factors in this score is the client age.\n",
    "\n",
    "Next we can look at the distribution of each of these features colored by the value of the target. This will let us visualize the effect of this variable on the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5e2b6507-96d1-4f96-964f-d8241e321f09",
    "_uuid": "49afab6b3790abcc2dea04c483f462f39e536503"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 12))\n",
    "\n",
    "# iterate through the sources\n",
    "for i, source in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):\n",
    "    \n",
    "    # create a new subplot for each source\n",
    "    plt.subplot(3, 1, i + 1)\n",
    "    # plot repaid loans\n",
    "    sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, source], label = 'target == 0')\n",
    "    # plot loans that were not repaid\n",
    "    sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, source], label = 'target == 1')\n",
    "    \n",
    "    # Label the plots\n",
    "    plt.title('Distribution of %s by Target Value' % source)\n",
    "    plt.xlabel('%s' % source); plt.ylabel('Density');\n",
    "    \n",
    "plt.tight_layout(h_pad = 2.5)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0ee531e8-f131-4ae3-b542-d4bf550d9bd5",
    "_uuid": "71ce5855665256dacfd7c52bceb11c68f5c58759"
   },
   "source": [
    "`EXT_SOURCE_3` displays the greatest difference between the values of the target. We can clearly see that this feature has some relationship to the likelihood of an applicant to repay a loan. The relationship is not very strong (in fact they are all [considered very weak](http://www.statstutor.ac.uk/resources/uploaded/pearsons.pdf), but these variables will still be useful for a machine learning model to predict whether or not an applicant will repay a loan on time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "53f486249f8afec0496d3de25120e57d956c2eb7"
   },
   "source": [
    "## Pairs Plot\n",
    "\n",
    "As a final exploratory plot, we can make a pairs plot of the `EXT_SOURCE` variables and the `DAYS_BIRTH` variable. The [Pairs Plot](https://towardsdatascience.com/visualizing-data-with-pair-plots-in-python-f228cf529166) is a great exploration tool because it lets us see relationships between multiple pairs of variables as well as distributions of single variables. Here we are using the seaborn visualization library and the PairGrid function to create a Pairs Plot with scatterplots on the upper triangle, histograms on the diagonal, and 2D kernel density plots and correlation coefficients on the lower triangle.\n",
    "\n",
    "If you don't understand this code, that's all right! Plotting in Python can be overly complex, and for anything beyond the simplest graphs, I usually find an existing implementation and adapt the code (don't repeat yourself)! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7b185a4e-ac04-4ff2-b5cb-46eacf6a70b6",
    "_uuid": "9400f9d2810f4331005c9b91e040818279d1eaf8"
   },
   "outputs": [],
   "source": [
    "# Copy the data for plotting\n",
    "plot_data = ext_data.drop(columns = ['DAYS_BIRTH']).copy()\n",
    "\n",
    "# Add in the age of the client in years\n",
    "plot_data['YEARS_BIRTH'] = age_data['YEARS_BIRTH']\n",
    "\n",
    "# Drop na values and limit to first 100000 rows\n",
    "plot_data = plot_data.dropna().loc[:100000, :]\n",
    "\n",
    "# Function to calculate correlation coefficient between two columns\n",
    "def corr_func(x, y, **kwargs):\n",
    "    r = np.corrcoef(x, y)[0][1]\n",
    "    ax = plt.gca()\n",
    "    ax.annotate(\"r = {:.2f}\".format(r),\n",
    "                xy=(.2, .8), xycoords=ax.transAxes,\n",
    "                size = 20)\n",
    "\n",
    "# Create the pairgrid object\n",
    "grid = sns.PairGrid(data = plot_data, size = 3, diag_sharey=False,\n",
    "                    hue = 'TARGET', \n",
    "                    vars = [x for x in list(plot_data.columns) if x != 'TARGET'])\n",
    "\n",
    "# Upper is a scatter plot\n",
    "grid.map_upper(plt.scatter, alpha = 0.2)\n",
    "\n",
    "# Diagonal is a histogram\n",
    "grid.map_diag(sns.kdeplot)\n",
    "\n",
    "# Bottom is density plot\n",
    "grid.map_lower(sns.kdeplot, cmap = plt.cm.OrRd_r);\n",
    "\n",
    "plt.suptitle('Ext Source and Age Features Pairs Plot', size = 32, y = 1.05);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "839f51f5-02f4-472d-9de4-aa2f760c171c",
    "_uuid": "88f9f486c74856bd87ff7699998088d9ee7fd926"
   },
   "source": [
    "In this plot, the red indicates loans that were not repaid and the blue are loans that are paid. We can see the different relationships within the data. There does appear to be a moderate positive linear relationship between the `EXT_SOURCE_1` and the `DAYS_BIRTH` (or equivalently `YEARS_BIRTH`), indicating that this feature may take into account the age of the client. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bd49d18b-e35f-4122-a005-dd06d8f2f7ca",
    "_uuid": "d5506d0483af10dbf71e8ed11c99b2d5253680fb"
   },
   "source": [
    "# Feature Engineering\n",
    "in this notebook we will try only two simple feature construction methods: \n",
    "\n",
    "* Polynomial features\n",
    "* Domain knowledge features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "464705a1-7ecf-47ba-a1fe-9f870102eb85",
    "_uuid": "70322dd11709dcaaf879a56103fde8fc787b7d4c"
   },
   "source": [
    "## Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e5b0efd9-67ac-4aa0-91e9-2141a87a6a8a",
    "_uuid": "a63d53dcac14c4ac2e31ea9c5e16b5d161c2415b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a new dataframe for polynomial features\n",
    "poly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', \n",
    "                           'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\n",
    "poly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2',\n",
    "                               'EXT_SOURCE_3', 'DAYS_BIRTH']]\n",
    "\n",
    "# imputer for handling missing values\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(strategy = 'median')\n",
    "\n",
    "poly_target = poly_features['TARGET']\n",
    "\n",
    "poly_features = poly_features.drop(columns = ['TARGET'])\n",
    "\n",
    "# Need to impute missing values\n",
    "poly_features = imputer.fit_transform(poly_features)\n",
    "poly_features_test = imputer.transform(poly_features_test)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "                                  \n",
    "# Create the polynomial object with specified degree\n",
    "poly_transformer = PolynomialFeatures(degree = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2be7c1ab-d1e5-40f2-b8e7-e2b2ce1e2f9a",
    "_uuid": "72c5ecaae9c6ff038d16cbd9208f1abb69912631"
   },
   "outputs": [],
   "source": [
    "# Train the polynomial features\n",
    "poly_transformer.fit(poly_features)\n",
    "\n",
    "# Transform the features\n",
    "poly_features = poly_transformer.transform(poly_features)\n",
    "poly_features_test = poly_transformer.transform(poly_features_test)\n",
    "print('Polynomial Features shape: ', poly_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a7833b1e-714c-4988-8cbf-757d01290d8f",
    "_uuid": "4d837e47bada5411ffce06266605f043c6ffe19e"
   },
   "source": [
    "This creates a considerable number of new features. To get the names we have to use the polynomial features `get_feature_names` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7465d1e6-d360-4029-afa7-67cb34f60249",
    "_uuid": "121f98d2ec9c81c5dabb911dc68562d0b2b6d737"
   },
   "outputs": [],
   "source": [
    "poly_transformer.get_feature_names(input_features = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7eaeb645-bb25-4ac5-884f-d1231ab6d88f",
    "_uuid": "4e68b80b2738ef46863b53b7b781f299d602d316"
   },
   "source": [
    "There are 35 features with individual features raised to powers up to degree 3 and interaction terms. Now, we can see whether any of these new features are correlated with the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "95725a63-f8f2-4680-8f7a-4252f04e7f7f",
    "_uuid": "e712923de757457bb87a35ecaccd27007b351e6c"
   },
   "outputs": [],
   "source": [
    "# Create a dataframe of the features \n",
    "poly_features = pd.DataFrame(poly_features, \n",
    "        columns = poly_transformer.get_feature_names(\n",
    "            ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']))\n",
    "\n",
    "# Add in the target\n",
    "poly_features['TARGET'] = poly_target\n",
    "\n",
    "# Find the correlations with the target\n",
    "poly_corrs = poly_features.corr()['TARGET'].sort_values()\n",
    "\n",
    "# Display most negative and most positive\n",
    "print(poly_corrs.head(10))\n",
    "print(poly_corrs.tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "971de432-c65e-4c9a-a3c0-c923ed27ddcb",
    "_uuid": "082ac97a068afed6758ed191acf5ab485e39230c"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ed758ed436a86f92a8ee574999aa91089242ca7a"
   },
   "outputs": [],
   "source": [
    "# Put test features into dataframe\n",
    "poly_features_test = pd.DataFrame(poly_features_test, \n",
    "                    columns = poly_transformer.get_feature_names(\n",
    "            ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']))\n",
    "\n",
    "# Merge polynomial features into training dataframe\n",
    "poly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']\n",
    "app_train_poly = app_train.merge(poly_features, \n",
    "                                 on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "# Merge polnomial features into testing dataframe\n",
    "poly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']\n",
    "app_test_poly = app_test.merge(poly_features_test, \n",
    "                               on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "# Align the dataframes\n",
    "app_train_poly, app_test_poly = app_train_poly.align(\n",
    "                        app_test_poly, join = 'inner', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9b27fad1522263c32b57a8127c84ad0e08ff9d8f"
   },
   "source": [
    "## Domain Knowledge Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c8d4b165b45da6c3120911de18e9348d8726c70c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "app_train_domain = app_train.copy()\n",
    "app_test_domain = app_test.copy()\n",
    "\n",
    "app_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT'] / app_train_domain['AMT_INCOME_TOTAL']\n",
    "app_train_domain['ANNUITY_INCOME_PERCENT'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_INCOME_TOTAL']\n",
    "app_train_domain['CREDIT_TERM'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_CREDIT']\n",
    "app_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED'] / app_train_domain['DAYS_BIRTH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d017103871bd4935a8c29599d6be33e0e74b2f83",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "app_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT'] / app_test_domain['AMT_INCOME_TOTAL']\n",
    "app_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_INCOME_TOTAL']\n",
    "app_test_domain['CREDIT_TERM'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_CREDIT']\n",
    "app_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] / app_test_domain['DAYS_BIRTH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7e917d654c05bd0ca3251d4f51c8176d82fe613f"
   },
   "source": [
    "#### Visualize New Variables\n",
    "\n",
    "We should explore these __domain knowledge__ variables visually in a graph. For all of these, we will make the same KDE plot colored by the value of the `TARGET`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e9c10d7f55b4c636335f815762b93598fe4acb0a"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 20))\n",
    "# iterate through the new features\n",
    "for i, feature in enumerate(['CREDIT_INCOME_PERCENT', 'ANNUITY_INCOME_PERCENT', 'CREDIT_TERM', 'DAYS_EMPLOYED_PERCENT']):\n",
    "    \n",
    "    # create a new subplot for each source\n",
    "    plt.subplot(4, 1, i + 1)\n",
    "    # plot repaid loans\n",
    "    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 0, feature], label = 'target == 0')\n",
    "    # plot loans that were not repaid\n",
    "    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 1, feature], label = 'target == 1')\n",
    "    \n",
    "    # Label the plots\n",
    "    plt.title('Distribution of %s by Target Value' % feature)\n",
    "    plt.xlabel('%s' % feature); plt.ylabel('Density');\n",
    "    \n",
    "plt.tight_layout(h_pad = 2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e27d400f3ec5447cfe5e908952351f271d521784"
   },
   "source": [
    "It's hard to say ahead of time if these new features will be useful. The only way to tell for sure is to try them out! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ebb64e63-6222-4509-a43c-302c6435ce09",
    "_uuid": "8bf057e523b2d99833f6dc9d95fe6141fb4e325a"
   },
   "source": [
    "# Baseline\n",
    "\n",
    "For a naive baseline, we could guess the same value for all examples on the testing set.  We are asked to predict the probability of not repaying the loan, so if we are entirely unsure, we would guess 0.5 for all observations on the test set. This  will get us a Reciever Operating Characteristic Area Under the Curve (AUC ROC) of 0.5 in the competition ([random guessing on a classification task will score a 0.5](https://stats.stackexchange.com/questions/266387/can-auc-roc-be-between-0-0-5)).\n",
    "\n",
    "Since we already know what score we are going to get, we don't really need to make a naive baseline guess. Let's use a slightly more sophisticated model for our actual baseline: Logistic Regression.\n",
    "\n",
    "## Logistic Regression Implementation\n",
    "\n",
    "Here I will focus on implementing the model rather than explaining the details, but for those who want to learn more about the theory of machine learning algorithms, I recommend both [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/) and [Hands-On Machine Learning with Scikit-Learn and TensorFlow](http://shop.oreilly.com/product/0636920052289.do). Both of these books present the theory and also the code needed to make the models (in R and Python respectively). They both teach with the mindset that the best way to learn is by doing, and they are very effective! \n",
    "\n",
    "To get a baseline, we will use all of the features after encoding the categorical variables. We will preprocess the data by filling in the missing values (imputation) and normalizing the range of the features (feature scaling). The following code performs both of these preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "60ef8744-ca3a-4810-8439-2835fbfc1833",
    "_uuid": "784ae2f91cf7792702595a9973ba773b2acdec00"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Imputer\n",
    "\n",
    "# Drop the target from the training data\n",
    "if 'TARGET' in app_train:\n",
    "    train = app_train.drop(columns = ['TARGET'])\n",
    "else:\n",
    "    train = app_train.copy()\n",
    "    \n",
    "# Feature names\n",
    "features = list(train.columns)\n",
    "\n",
    "# Copy of the testing data\n",
    "test = app_test.copy()\n",
    "\n",
    "# Median imputation of missing values\n",
    "imputer = Imputer(strategy = 'median')\n",
    "\n",
    "# Scale each feature to 0-1\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "\n",
    "# Fit on the training data\n",
    "imputer.fit(train)\n",
    "\n",
    "# Transform both training and testing data\n",
    "train = imputer.transform(train)\n",
    "test = imputer.transform(app_test)\n",
    "\n",
    "# Repeat with the scaler\n",
    "scaler.fit(train)\n",
    "train = scaler.transform(train)\n",
    "test = scaler.transform(test)\n",
    "\n",
    "print('Training data shape: ', train.shape)\n",
    "print('Testing data shape: ', test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1bcfab25-cc1c-4553-9473-96fcfeb2a61a",
    "_uuid": "364f0835a46f7a7bb7be487b54d92f5ff50ed341"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6462ff85-e3b6-4a5f-b95c-9416841413b1",
    "_uuid": "9e8aba9401e8367f9902d710ba49e820294870e1"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Make the model with the specified regularization parameter\n",
    "log_reg = LogisticRegression(C = 0.0001)\n",
    "\n",
    "# Train on the training data\n",
    "log_reg.fit(train, train_labels)\n",
    "\n",
    "log_reg_pred = log_reg.predict_proba(test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3a2612e95b13a94a13f679c1754b6c4fb28c332d"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "09a3d281e4c7ee6820f402e32f31775851113089"
   },
   "outputs": [],
   "source": [
    "# Submission dataframe\n",
    "submit = app_test[['SK_ID_CURR']]\n",
    "submit['TARGET'] = log_reg_pred\n",
    "\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2a1bf4f54df8b37a71a7732e61a7bfebafd8be11"
   },
   "source": [
    "\n",
    "\n",
    "## Improved Model: Random Forest\n",
    "\n",
    "To try and beat the poor performance of our baseline, we can update the algorithm. Let's try using a Random Forest on the same training data to see how that affects performance. The Random Forest is a much more powerful model especially when we use hundreds of trees. We will use 100 trees in the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6643479e-7980-431c-a6a2-9087acdb0f42",
    "_uuid": "cf05e2318904b8f3575ae233c185cd995fd07643",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Make the random forest classifier\n",
    "random_forest = RandomForestClassifier(n_estimators = 100, \n",
    "                            random_state = 50, verbose = 1, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "020f0856-8f24-4b22-bca5-aac7f137f032",
    "_uuid": "52258a9b89b3069bc1d82829107e8e7c1ef05fd6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train on the training data\n",
    "random_forest.fit(train, train_labels)\n",
    "\n",
    "# Extract feature importances\n",
    "feature_importance_values = random_forest.feature_importances_\n",
    "feature_importances = pd.DataFrame({'feature': features, \n",
    "                        'importance': feature_importance_values})\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = random_forest.predict_proba(test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "25145966-669e-426d-89a3-98e30b861057",
    "_uuid": "1da4b02502388d2b8a2bc5376027c5bef50272f3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a submission dataframe\n",
    "submit = app_test[['SK_ID_CURR']]\n",
    "submit['TARGET'] = predictions\n",
    "\n",
    "# Save the submission dataframe\n",
    "submit.to_csv('random_forest_baseline.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cf6f600ed10c511dd26d4bd5efa7997ab8d6916a"
   },
   "source": [
    "### Make Predictions using Engineered Features\n",
    "\n",
    "The only way to see if the Polynomial Features and Domain knowledge improved the model is to train a test a model on these features! We can then compare the submission performance to that for the model without these features to gauge the effect of our feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d9d49008fb73b8d15c797850c64d5e6f81375163",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poly_features_names = list(app_train_poly.columns)\n",
    "\n",
    "# Impute the polynomial features\n",
    "imputer = Imputer(strategy = 'median')\n",
    "\n",
    "poly_features = imputer.fit_transform(app_train_poly)\n",
    "poly_features_test = imputer.transform(app_test_poly)\n",
    "\n",
    "# Scale the polynomial features\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "\n",
    "poly_features = scaler.fit_transform(poly_features)\n",
    "poly_features_test = scaler.transform(poly_features_test)\n",
    "\n",
    "random_forest_poly = RandomForestClassifier(n_estimators = 100, \n",
    "                        random_state = 50, verbose = 1, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a7d3f3b6cdf8231832c56224c8a694056e456593",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_forest_poly.fit(poly_features, train_labels)\n",
    "predictions = random_forest_poly.predict_proba(poly_features_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cd923eed057b6d61354db27473d9a36f1411dd5c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a submission dataframe\n",
    "submit = app_test[['SK_ID_CURR']]\n",
    "submit['TARGET'] = predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ec50627c874a9d78d6789e01a47e829c820f9615"
   },
   "source": [
    "This model scored 0.678 when submitted to the competition, exactly the same as that without the engineered features. Given these results, it does not appear that our feature construction helped in this case. \n",
    "\n",
    "#### Testing Domain Features\n",
    "\n",
    "Now we can test the domain features we made by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "04b93e7d3629c1a5ba27a6eed037900862dc039d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "app_train_domain = app_train_domain.drop(columns = 'TARGET')\n",
    "\n",
    "domain_features_names = list(app_train_domain.columns)\n",
    "\n",
    "# Impute the domainnomial features\n",
    "imputer = Imputer(strategy = 'median')\n",
    "\n",
    "domain_features = imputer.fit_transform(app_train_domain)\n",
    "domain_features_test = imputer.transform(app_test_domain)\n",
    "\n",
    "# Scale the domainnomial features\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "\n",
    "domain_features = scaler.fit_transform(domain_features)\n",
    "domain_features_test = scaler.transform(domain_features_test)\n",
    "\n",
    "random_forest_domain = RandomForestClassifier(n_estimators = 100,\n",
    "                            random_state = 50, verbose = 1, n_jobs = -1)\n",
    "\n",
    "# Train on the training data\n",
    "random_forest_domain.fit(domain_features, train_labels)\n",
    "\n",
    "# Extract feature importances\n",
    "feature_importance_domain = random_forest_domain.feature_importances_\n",
    "feature_importances_domain = pd.DataFrame({'feature': domain_features_names,\n",
    "                        'importance': feature_importance_values_domain})\n",
    "\n",
    "predictions = random_forest_domain.predict_proba(domain_features_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "27598fb499df4c3282be63356422e4a6f6d6dd17",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a submission dataframe\n",
    "submit = app_test[['SK_ID_CURR']]\n",
    "submit['TARGET'] = predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "240fb8ba2b5fe73da4d021543fd64baa104fb418"
   },
   "source": [
    "## Model Interpretation: Feature Importances\n",
    "\n",
    "As a simple method to see which variables are the most relevant, we can look at the feature importances of the random forest. Given the correlations we saw in the exploratory data analysis, we should expect that the most important features are the `EXT_SOURCE` and the `DAYS_BIRTH`. We may use these feature importances as a method of dimensionality reduction in future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a90e9368-5f7d-4179-a5cc-1025f32c6a81",
    "_uuid": "b912337a5f35f495398d8ae8b8576ceb7062fe50",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_feature_importances(df):\n",
    "    df = df.sort_values('importance', ascending = False).reset_index()\n",
    "    \n",
    "    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n",
    "\n",
    "    plt.figure(figsize = (10, 6))\n",
    "    ax = plt.subplot()\n",
    "    \n",
    "    ax.barh(list(reversed(list(df.index[:15]))), \n",
    "            df['importance_normalized'].head(15), \n",
    "            align = 'center', edgecolor = 'k')\n",
    "    \n",
    "    ax.set_yticks(list(reversed(list(df.index[:15]))))\n",
    "    ax.set_yticklabels(df['feature'].head(15))\n",
    "    \n",
    "    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n",
    "    plt.show()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1084ad42-bc44-438b-b2fd-5fd7a1c1b363",
    "_uuid": "37309c4a94b248ad85fa7a0825f01830a818ba92",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Show the feature importances for the default features\n",
    "feature_importances_sorted = plot_feature_importances(feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "524c6aa12acc80e7018750e7a8897dc6b4bacf18"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fd407ca0f7c5c50ee71fe5c8532eabeb92c15c50"
   },
   "source": [
    "# Conclusions\n",
    "\n",
    "In this notebook, we saw how to get started with a Kaggle machine learning competition. We first made sure to understand the data, our task, and the metric by which our submissions will be judged. Then, we performed a fairly simple EDA to try and identify relationships, trends, or anomalies that may help our modeling. Along the way, we performed necessary preprocessing steps such as encoding categorical variables, imputing missing values, and scaling features to a range. Then, we constructed new features out of the existing data to see if doing so could help our model. \n",
    "\n",
    "Once the data exploration, data preparation, and feature engineering was complete, we implemented a baseline model upon which we hope to improve. Then we built a second slightly more complicated model to beat our first score. We also carried out an experiment to determine the effect of adding the engineering variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d12452cd-347e-4269-b3d4-f5f0589f4c5c",
    "_uuid": "a8bc307f9be27bfabbc3891deddbd94293ca03fa"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "60208a3f-947f-42d9-8f46-2159afd2eb7d",
    "_uuid": "2719663ed461422fce26b5dd55a31ab9718df47a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "\n",
    "def model(features, test_features, encoding='ohe', n_folds=5):\n",
    "    \n",
    "    train_ids = features['SK_ID_CURR']\n",
    "    test_ids = test_features['SK_ID_CURR']\n",
    "    labels = features['TARGET']\n",
    "    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n",
    "    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n",
    "    \n",
    "    if encoding == 'ohe':\n",
    "        features = pd.get_dummies(features)\n",
    "        test_features = pd.get_dummies(test_features)\n",
    "        features, test_features = features.align(test_features, \n",
    "                                join = 'inner', axis = 1)\n",
    "        cat_indices = 'auto'\n",
    "    elif encoding == 'le':\n",
    "        label_encoder = LabelEncoder()\n",
    "        cat_indices = []\n",
    "        for i, col in enumerate(features):\n",
    "            if features[col].dtype == 'object':\n",
    "                features[col] = label_encoder.fit_transform(\n",
    "                    np.array(features[col].astype(str)).reshape((-1,)))\n",
    "                test_features[col] = label_encoder.transform(\n",
    "                    np.array(test_features[col].astype(str)).reshape((-1,)))\n",
    "                cat_indices.append(i)\n",
    "    else:\n",
    "        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")    \n",
    "    feature_names = list(features.columns)\n",
    "    features = np.array(features)\n",
    "    test_features = np.array(test_features)\n",
    "    \n",
    "    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n",
    "    \n",
    "    feature_importance_values = np.zeros(len(feature_names))    \n",
    "    test_predictions = np.zeros(test_features.shape[0])\n",
    "    out_of_fold = np.zeros(features.shape[0])\n",
    "    \n",
    "    valid_scores = []\n",
    "    train_scores = []\n",
    "    for train_indices, valid_indices in k_fold.split(features):\n",
    "        \n",
    "        train_features, train_labels = features[train_indices], labels[train_indices]\n",
    "        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n",
    "        \n",
    "        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary',\n",
    "                        class_weight = 'balanced', learning_rate = 0.05, \n",
    "                        reg_alpha = 0.1, reg_lambda = 0.1, \n",
    "                        subsample = 0.8, n_jobs = -1, random_state = 50)\n",
    "        \n",
    "        model.fit(train_features, train_labels, eval_metric='auc',\n",
    "                  eval_set=[(valid_features, valid_labels), \n",
    "                              (train_features, train_labels)],\n",
    "                  eval_names=['valid', 'train'], \n",
    "                  categorical_feature=cat_indices,\n",
    "                  early_stopping_rounds=100, verbose=200)\n",
    "        \n",
    "        best_iteration = model.best_iteration_\n",
    "        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n",
    "        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n",
    "        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n",
    "        valid_score = model.best_score_['valid']['auc']\n",
    "        train_score = model.best_score_['train']['auc']\n",
    "        valid_scores.append(valid_score)\n",
    "        train_scores.append(train_score)\n",
    "        gc.enable()\n",
    "        del model, train_features, valid_features\n",
    "        gc.collect()\n",
    "        \n",
    "    submission = pd.DataFrame({'SK_ID_CURR': test_ids, \n",
    "                               'TARGET': test_predictions})\n",
    "    \n",
    "    feature_importances = pd.DataFrame({'feature': feature_names, \n",
    "                                        'importance': feature_importance_values})\n",
    "    \n",
    "    valid_auc = roc_auc_score(labels, out_of_fold)\n",
    "    valid_scores.append(valid_auc)\n",
    "    train_scores.append(np.mean(train_scores))\n",
    "    fold_names = list(range(n_folds))\n",
    "    fold_names.append('overall')\n",
    "    metrics = pd.DataFrame({'fold': fold_names,'train': train_scores,\n",
    "                            'valid': valid_scores}) \n",
    "    \n",
    "    return submission, feature_importances, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "89e02dcbb23e47e3504ed1f61431b182e2011ba5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission, fi, metrics = model(app_train, app_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ca59467edd1060e5f7587a77a31dcd7331ce90ec",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a7e9a1149953069853d4d83ec46f22084dce8711",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
