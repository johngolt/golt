{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T02:02:54.918165Z",
     "start_time": "2019-10-08T02:02:54.393145Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T02:02:55.026039Z",
     "start_time": "2019-10-08T02:02:54.995149Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.random.randint(0,4,100)\n",
    "y = np.random.randint(4,7,100)\n",
    "df = pd.DataFrame({'x':x,'y':y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T08:47:42.636061Z",
     "start_time": "2019-10-08T08:47:42.623065Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T08:47:43.527870Z",
     "start_time": "2019-10-08T08:47:43.249331Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T06:08:42.204088Z",
     "start_time": "2019-09-12T06:08:37.529116Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "x = np.arange(-5, 5, 0.1)\n",
    "y = stats.norm.cdf(x, 0, 1)\n",
    "plt.plot(x, y)\n",
    "import pandas as pd\n",
    "#绘制目标数据（这里使用UCI机器学习数据库中的churn数据集）的累计分布函数图\n",
    "churn_raw_data = pd.read_csv('churn.txt')\n",
    "day_minute = churn_raw_data['Day Mins']\n",
    "sorted_ = np.sort(day_minute)\n",
    "yvals = np.arange(len(sorted_))/float(len(sorted_))\n",
    "plt.plot(sorted_, yvals)\n",
    "x_label = stats.norm.ppf(yvals)  #对目标累计分布函数值求标准正太分布累计分布函数的逆\n",
    "plt.scatter(x_label, sorted_)\n",
    "stats.probplot(day_minute, dist=\"norm\", plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T06:13:03.673491Z",
     "start_time": "2019-09-12T06:13:03.669525Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 第一步:构造DataFrame数据\n",
    "time_stamps = ['2015-03-08 10:30:00.360000+00:00', '2017-07-13 15:45:05.755000-07:00',\n",
    "               '2012-01-20 22:30:00.254000+05:30', '2016-12-25 00:30:00.000000+10:00']\n",
    "\n",
    "# 第二步: 将time_stamps转换为DataFrame格式\n",
    "time_pd = pd.DataFrame(time_stamps, columns=['Times'])\n",
    "\n",
    "# 第三步: 使用pd.Timestamp 将字符串类型转换为日期格式\n",
    "time_pd['stamp'] = [pd.Timestamp(time) for time in time_pd['Times'].values]\n",
    "# print(time_pd[['stamp', 'Times']])\n",
    "# 第五步： 提取与时刻有关的特征\n",
    "time_pd['Hour'] = time_pd['stamp'].apply(lambda d: d.hour)\n",
    "time_pd['Minute'] = time_pd['stamp'].apply(lambda d: d.minute)\n",
    "time_pd['Second'] = time_pd['stamp'].apply(lambda d: d.second)\n",
    "time_pd['MUsecond'] = time_pd['stamp'].apply(lambda d: d.microsecond)   #毫秒\n",
    "time_pd['UTC_offset'] = time_pd['stamp'].apply(lambda d: d.utcoffset())\n",
    "\n",
    "# 第六步：使用pd.cut将hour的数据进行切分，分成几个过程\n",
    "cut_hour = [-1, 5, 11, 16, 21, 23]\n",
    "cut_labels = ['last night', 'morning', 'afternoon', 'evening', 'Night']\n",
    "time_pd['Hour_cut'] = pd.cut(time_pd['Hour'], bins=cut_hour, labels=cut_labels)\n",
    "print(time_pd['Hour_cut'].head())\n",
    "# 第七步：使用LabelEncoder对标签进行数值转换\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "La = LabelEncoder()\n",
    "time_pd['Hour_number'] = La.fit_transform(time_pd['Hour_cut'])\n",
    "label_dict = {classes: number for number, classes in enumerate(La.classes_)}\n",
    "print(time_pd[['Hour_cut', 'Hour_number']])\n",
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T06:13:15.361672Z",
     "start_time": "2019-09-12T06:13:15.298168Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 3, 4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from itertools import product\n",
    "\n",
    "class MeanEncoder:\n",
    "    def __init__(self, categorical_features, n_splits=5, \n",
    "                 target_type='classification', prior_weight_func=None):\n",
    "\n",
    "        self.categorical_features = categorical_features\n",
    "        self.n_splits = n_splits\n",
    "        self.learned_stats = {}\n",
    "\n",
    "        if target_type == 'classification':\n",
    "            self.target_type = target_type\n",
    "            self.target_values = []\n",
    "        else:\n",
    "            self.target_type = 'regression'\n",
    "            self.target_values = None\n",
    "\n",
    "        if isinstance(prior_weight_func, dict):\n",
    "            self.prior_weight_func = eval('lambda x: 1 / (1 + np.exp((x - k) / f))', dict(prior_weight_func, np=np))\n",
    "        elif callable(prior_weight_func):\n",
    "            self.prior_weight_func = prior_weight_func\n",
    "        else:\n",
    "            self.prior_weight_func = lambda x: 1 / (1 + np.exp((x - 2) / 1))\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_encode_subroutine(X_train, y_train, X_test, variable, target, prior_weight_func):\n",
    "        X_train = X_train[[variable]].copy()\n",
    "        X_test = X_test[[variable]].copy()\n",
    "\n",
    "        if target is not None:\n",
    "            nf_name = '{}_pred_{}'.format(variable, target)\n",
    "            X_train['pred_temp'] = (y_train == target).astype(int)  # classification\n",
    "        else:\n",
    "            nf_name = '{}_pred'.format(variable)\n",
    "            X_train['pred_temp'] = y_train  # regression\n",
    "        prior = X_train['pred_temp'].mean()\n",
    "\n",
    "        col_avg_y = X_train.groupby(by=variable, axis=0)['pred_temp'].agg({'mean': 'mean', 'beta': 'size'})\n",
    "        col_avg_y['beta'] = prior_weight_func(col_avg_y['beta'])\n",
    "        col_avg_y[nf_name] = col_avg_y['beta'] * prior + (1 - col_avg_y['beta']) * col_avg_y['mean']\n",
    "        col_avg_y.drop(['beta', 'mean'], axis=1, inplace=True)\n",
    "\n",
    "        nf_train = X_train.join(col_avg_y, on=variable)[nf_name].values\n",
    "        nf_test = X_test.join(col_avg_y, on=variable).fillna(prior, inplace=False)[nf_name].values\n",
    "\n",
    "        return nf_train, nf_test, prior, col_avg_y\n",
    "\n",
    "    def fit_transform(self, X, y):\n",
    "        X_new = X.copy()\n",
    "        if self.target_type == 'classification':\n",
    "            skf = StratifiedKFold(self.n_splits)\n",
    "        else:\n",
    "            skf = KFold(self.n_splits)\n",
    "\n",
    "        if self.target_type == 'classification':\n",
    "            self.target_values = sorted(set(y))\n",
    "            self.learned_stats = {'{}_pred_{}'.format(variable, target): [] for variable, target in\n",
    "                                  product(self.categorical_features, self.target_values)}\n",
    "            for variable, target in product(self.categorical_features, self.target_values):\n",
    "                nf_name = '{}_pred_{}'.format(variable, target)\n",
    "                X_new.loc[:, nf_name] = np.nan\n",
    "                for large_ind, small_ind in skf.split(y, y):\n",
    "                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(\n",
    "                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, target, self.prior_weight_func)\n",
    "                    X_new.iloc[small_ind, -1] = nf_small\n",
    "                    self.learned_stats[nf_name].append((prior, col_avg_y))\n",
    "        else:\n",
    "            self.learned_stats = {'{}_pred'.format(variable): [] for variable in self.categorical_features}\n",
    "            for variable in self.categorical_features:\n",
    "                nf_name = '{}_pred'.format(variable)\n",
    "                X_new.loc[:, nf_name] = np.nan\n",
    "                for large_ind, small_ind in skf.split(y, y):\n",
    "                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(\n",
    "                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, None, self.prior_weight_func)\n",
    "                    X_new.iloc[small_ind, -1] = nf_small\n",
    "                    self.learned_stats[nf_name].append((prior, col_avg_y))\n",
    "        return X_new\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_new = X.copy()\n",
    "\n",
    "        if self.target_type == 'classification':\n",
    "            for variable, target in product(self.categorical_features, self.target_values):\n",
    "                nf_name = '{}_pred_{}'.format(variable, target)\n",
    "                X_new[nf_name] = 0\n",
    "                for prior, col_avg_y in self.learned_stats[nf_name]:\n",
    "                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[\n",
    "                        nf_name]\n",
    "                X_new[nf_name] /= self.n_splits\n",
    "        else:\n",
    "            for variable in self.categorical_features:\n",
    "                nf_name = '{}_pred'.format(variable)\n",
    "                X_new[nf_name] = 0\n",
    "                for prior, col_avg_y in self.learned_stats[nf_name]:\n",
    "                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[\n",
    "                        nf_name]\n",
    "                X_new[nf_name] /= self.n_splits\n",
    "\n",
    "        return X_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeivdedByZero(nominator, denominator):\n",
    "    if denominator == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return nominator*1.0/denominator\n",
    "time_window = [7, 30, 60, 90, 120, 150, 180]\n",
    "var_list = ['LogInfo1','LogInfo2']\n",
    "data1GroupbyIdx = pd.DataFrame({\n",
    "    'Idx':data1_train['Idx'].drop_duplicates()})\n",
    "\n",
    "for tw in time_window:\n",
    "    data1_train['TruncatedLogInfo'] = data1_train['Listinginfo'].map(\n",
    "        lambda x: x + datetime.timedelta(-tw))\n",
    "    temp = data1_train.loc[data1_train['logInfo'] >= data1_train['TruncatedLogInfo']]\n",
    "    for var in var_list:\n",
    "        #count the frequences of LogInfo1 and LogInfo2\n",
    "        count_stats = temp.groupby(['Idx'])[var].count().to_dict()\n",
    "        data1GroupbyIdx[str(var)+'_'+str(tw)+'_ct'] = \n",
    "        data1GroupbyIdx['Idx'].map(lambda x: \n",
    "                                        count_stats.get(x,0))\n",
    "\n",
    "        # count the distinct value of LogInfo1 and LogInfo2\n",
    "        Idx_UserupdateInfo1 = temp[['Idx', var]].drop_duplicates()\n",
    "        uniq_stats = Idx_UserupdateInfo1.groupby(['Idx'])[var].count(\n",
    "        ).to_dict()\n",
    "        data1GroupbyIdx[str(var) + '_' + str(tw) + '_uq'] = \n",
    "        data1GroupbyIdx['Idx'].map(lambda x: uniq_stats.get(x,0))\n",
    "\n",
    "        # calculate the average count of each value in LogInfo1 and LogInfo2\n",
    "        data1GroupbyIdx[str(var) + '_' + str(tw) + '_avg_ct'] = \n",
    "        data1GroupbyIdx[[str(var)+'_'+str(tw)+'_count',\n",
    "            str(var) + '_' + str(tw) + '_unique']].\\\n",
    "            apply(lambda x: DeivdedByZero(x[0],x[1]), axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
